import os
from typing import List, Dict, Any
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain.agents import create_openai_functions_agent, AgentExecutor
from dotenv import load_dotenv
import sys
sys.path.append('..')
from HR.tools.rag_tool import search_documents, calculate_retirement_pay, search_naver_news, search_naver_web, hybrid_search, search_company_info
import torch

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë„êµ¬ ëª©ë¡ - ìƒˆë¡œìš´ í†µí•© ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
tools = [
    hybrid_search,           # 1ìˆœìœ„: í†µí•© ê²€ìƒ‰ (ë‚´ë¶€ ìš°ì„ )
    search_company_info,     # 2ìˆœìœ„: íšŒì‚¬ ì „ìš© ê²€ìƒ‰
    calculate_retirement_pay, # 3ìˆœìœ„: í‡´ì§ê¸ˆ ê³„ì‚°
    search_naver_news,       # 4ìˆœìœ„: ë„¤ì´ë²„ ë‰´ìŠ¤
    search_documents,        # 5ìˆœìœ„: ê¸°ì¡´ ë¬¸ì„œ ê²€ìƒ‰ (í•˜ìœ„ í˜¸í™˜)
    search_naver_web         # 6ìˆœìœ„: ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰
]

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ë„êµ¬ ì‚¬ìš© ìš°ì„ ìˆœìœ„ ëª…í™•í™”
system_prompt = """
ë‹¹ì‹ ì€ íšŒì‚¬ ë‚´ë¶€ ì •ë³´ì™€ ì™¸ë¶€ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë„êµ¬ ì‚¬ìš© ìš°ì„ ìˆœìœ„ë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ê³ , ì—†ëŠ” ìë£ŒëŠ” ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.

ğŸ¯ **ë„êµ¬ ì‚¬ìš© ìš°ì„ ìˆœìœ„:**

**1ìˆœìœ„: hybrid_search** â­ (ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì— ì‚¬ìš©)
- ëª¨ë“  ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ìš°ì„  ì‚¬ìš©
- ë‚´ë¶€ ë¬¸ì„œë¥¼ ë¨¼ì € ê²€ìƒ‰í•˜ê³  í•„ìš”ì‹œ ì™¸ë¶€ ê²€ìƒ‰ ë³´ì™„
- "ì •ì£¼ì˜ì´ ëˆ„êµ¬ì•¼?", "AI ê¸°ìˆ  ë™í–¥" ë“± ëª¨ë“  ì§ˆë¬¸

**2ìˆœìœ„: search_company_info** ğŸ¢ (íšŒì‚¬ ì „ìš© ì •ë³´)
- íšŒì‚¬ ë‚´ë¶€ ì •ë³´ë§Œ í™•ì‹¤íˆ ì•Œê³  ì‹¶ì„ ë•Œ
- "ìš°ë¦¬ íšŒì‚¬ íœ´ê°€ ì •ì±…", "ì¡°ì§ë„", "ì¸ì‚¬ê·œì •" ë“±

**3ìˆœìœ„: calculate_retirement_pay** ğŸ’°
- í‡´ì§ê¸ˆ ê³„ì‚°ì´ ëª…í™•í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©

**4ìˆœìœ„: search_naver_news** ğŸ“°
- ìµœì‹  ë‰´ìŠ¤, ë™í–¥ì´ í•„ìš”í•  ë•Œ

**ë‚˜ë¨¸ì§€ ë„êµ¬ë“¤ì€ íŠ¹ë³„í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©**

ğŸ¯ **ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
- ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸: hybrid_search ì‚¬ìš© (ë‚´ë¶€ + ì™¸ë¶€ ì •ë³´ í†µí•©)
- íšŒì‚¬ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì‹œ: ë‚´ë¶€ ì •ë³´ ìš°ì„  í‘œì‹œ
- ì¶œì²˜ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
- "ì •ì£¼ì˜" ê°™ì€ ê²½ìš°: íšŒì‚¬ ëŒ€í‘œì™€ í˜„ëŒ€ ì°½ì—…ì ì •ë³´ ëª¨ë‘ ì œê³µ

ğŸ“ **ì˜ˆì‹œ:**
- "ì •ì£¼ì˜ì´ ëˆ„êµ¬ì•¼?" â†’ hybrid_search (íšŒì‚¬ ëŒ€í‘œ + í˜„ëŒ€ ì°½ì—…ì ì •ë³´)
- "ìš°ë¦¬ íšŒì‚¬ ëŒ€í‘œê°€ ëˆ„êµ¬ì•¼?" â†’ search_company_info (íšŒì‚¬ ì •ë³´ë§Œ)
- "ìµœê·¼ AI ë‰´ìŠ¤" â†’ search_naver_news (ë‰´ìŠ¤ ì „ìš©)
"""

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

# LLM ì´ˆê¸°í™” - ëª¨ë¸ ì„ íƒ ë¡œì§ ì¶”ê°€
def get_llm():
    """í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ ì ì ˆí•œ LLMì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    llm_model = os.getenv("LLM_MODEL", "gpt-4o-mini")
    temperature = float(os.getenv("TEMPERATURE", "0"))
    
    if llm_model.startswith("gpt"):
        # OpenAI ëª¨ë¸ ì‚¬ìš©
        return ChatOpenAI(
            model=llm_model,
            temperature=temperature,
            streaming=True,
            api_key=os.getenv("OPENAI_API_KEY")
        )
    elif "HyperCLOVAX" in llm_model or "naver-hyperclovax" in llm_model:
        # HyperCLOVAX ëª¨ë¸ ì‚¬ìš© (HuggingFace Transformers ê¸°ë°˜)
        try:
            from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            
            print(f"HyperCLOVAX ëª¨ë¸ ë¡œë“œ ì¤‘: {llm_model}")
            
            # HuggingFace í† í° ì‚¬ìš©
            hf_token = os.getenv("HUGGINGFACE_TOKEN")
            
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                llm_model, 
                token=hf_token if hf_token else None
            )
            model = AutoModelForCausalLM.from_pretrained(
                llm_model,
                token=hf_token if hf_token else None,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else "cpu"
            )
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            max_new_tokens = int(os.getenv("MAX_NEW_TOKENS", "1024"))
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                pad_token_id=tokenizer.eos_token_id
            )
            
            return HuggingFacePipeline(pipeline=pipe)
            
        except ImportError:
            print("HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("OpenAI ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return ChatOpenAI(
                model="gpt-4o-mini",
                temperature=temperature,
                streaming=True,
                api_key=os.getenv("OPENAI_API_KEY")
            )
        except Exception as e:
            print(f"HyperCLOVAX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("OpenAI ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return ChatOpenAI(
                model="gpt-4o-mini",
                temperature=temperature,
                streaming=True,
                api_key=os.getenv("OPENAI_API_KEY")
            )
    else:
        # ê¸°ë³¸ê°’: OpenAI GPT ëª¨ë¸
        return ChatOpenAI(
            model="gpt-4o-mini",
            temperature=temperature,
            streaming=True,
            api_key=os.getenv("OPENAI_API_KEY")
        )

llm = get_llm()

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_openai_functions_agent(llm, tools, prompt)

# ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„±
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
class ChatManager:
    def __init__(self):
        self.chat_history: List[Dict[str, Any]] = []

    def add_message(self, role: str, content: str):
        self.chat_history.append({"role": role, "content": content})

    def get_chat_history(self):
        return self.chat_history

    def clear_history(self):
        self.chat_history = []

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chat_manager = ChatManager()

def process_query(query: str) -> str:
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    chat_manager.add_message("user", query)
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    response = agent_executor.invoke({
        "input": query,
        "chat_history": chat_manager.get_chat_history()
    })
    
    # AI ì‘ë‹µ ì¶”ê°€
    chat_manager.add_message("assistant", response["output"])
    
    return response["output"] 