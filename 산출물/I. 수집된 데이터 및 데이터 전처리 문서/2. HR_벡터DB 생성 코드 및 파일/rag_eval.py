import json
from sentence_transformers import CrossEncoder
from langchain_community.vectorstores import FAISS
from evaluate import load

# ëª¨ë¸ ë¡œë”©
def load_reranker_model(path="work1/models/bge-reranker-v2-m3-ko"):
    return CrossEncoder(path, device="cuda")

def load_vectorstore(folder="faiss_seq", index_name="index", embedding=None):
    return FAISS.load_local(folder_path=folder, embeddings=embedding, index_name=index_name, allow_dangerous_deserialization=True)

def load_eval_questions(path="eval_questions_merged.jsonl"):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]

# Rerank
def rerank_documents(query, docs, reranker, top_k=1):
    pairs = [[query, doc.page_content] for doc in docs]
    scores = reranker.predict(pairs)
    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [doc.page_content for doc, _ in reranked[:top_k]]

# GPT í˜¸ì¶œ
def generate_answer(query, context, client, prompt_style="cot"):
    if prompt_style == "cot":
        system_prompt = (
            "ë„ˆëŠ” íšŒì‚¬ ê·œì •ì„ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ QA ë¹„ì„œì•¼.\n"
            "ë‹µë³€ ì „ì— ë¬¸ì„œ ë‚´ìš©ì„ í•œ ë‹¨ê³„ì”© ì •ë¦¬í•˜ê³ , ë…¼ë¦¬ì ìœ¼ë¡œ ë‹µì„ ë„ì¶œí•´.\n"
            "ë¬¸ì„œì— ëª…í™•í•œ ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´."
        )
    else:
        system_prompt = "ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì •ì— ëŒ€í•´ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."

    user_prompt = f"""ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0
    )

    return response.choices[0].message.content.strip()

# í‰ê°€
def evaluate_answer(prediction, reference, qid):
    f1 = load("evaluate-metric/squad", "f1")
    em = load("evaluate-metric/squad", "exact_match")

    pred_dict = {"id": qid, "prediction_text": prediction}
    ref_dict = {"id": qid, "answers": [{"text": reference, "answer_start": 0}]}

    f1_score = f1.compute(predictions=[pred_dict], references=[ref_dict])["f1"]
    em_score = em.compute(predictions=[pred_dict], references=[ref_dict])["exact_match"]
    return f1_score, em_score

# ì „ì²´ í‰ê°€ ë£¨í”„
def run_full_evaluation(client, embedding, reranker_path="work1/models/bge-reranker-v2-m3-ko", top_k=1, prompt_style="cot"):
    reranker = load_reranker_model(reranker_path)
    vectorstore = load_vectorstore(embedding=embedding)
    questions = load_eval_questions()

    all_f1, all_em = [], []

    for idx, qa in enumerate(questions):
        qid = qa.get("id", str(idx))
        query = qa["question"]
        answer = qa["answer"]

        docs = vectorstore.similarity_search(query, k=5)
        top_context = "\n\n".join(rerank_documents(query, docs, reranker, top_k=top_k))

        prediction = generate_answer(query, top_context, client, prompt_style=prompt_style)
        f1, em = evaluate_answer(prediction, answer, qid)

        all_f1.append(f1)
        all_em.append(em)

    print(f"ğŸ“Š í‰ê·  F1: {sum(all_f1)/len(all_f1):.2f}")
    print(f"ğŸ“Š í‰ê·  EM: {sum(all_em)/len(all_em):.2f}")
    

def generate_answer(query, context, client, prompt_style="cot"):
    if prompt_style == "basic":
        system_prompt = "ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì •ì— ëŒ€í•´ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."
    elif prompt_style == "strict":
        system_prompt = (
            "ë„ˆëŠ” íšŒì‚¬ì˜ ì‚¬ë‚´ ê·œì •ì„ ì •í™•íˆ ì•ˆë‚´í•˜ëŠ” QA ë¹„ì„œì•¼. "
            "ë¬¸ì„œì— ê¸°ë°˜í•´ì„œë§Œ ë‹µë³€í•˜ê³ , ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´."
        )
    elif prompt_style == "cot":
        system_prompt = (
            "ë„ˆëŠ” íšŒì‚¬ ê·œì •ì„ ì•ˆë‚´í•˜ëŠ” QA ë¹„ì„œì•¼. "
            "ë¬¸ì„œë¥¼ ì •ë¦¬í•˜ê³ , ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ ë’¤ ë‹µë³€í•´. "
            "ê·¼ê±° ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  í•´."
        )
    elif prompt_style == "step":
        system_prompt = (
            "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì „ ë‹¨ê³„ë¥¼ ë‚˜ëˆ ì„œ ì„¤ëª…í•œ ë’¤, ê²°ë¡ ì„ ë‚´ë ¤ì£¼ì„¸ìš”."
        )
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_style}")

    user_prompt = f"""ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0
    )

    return response.choices[0].message.content.strip()
  
def run_prompt_ab_test(client, embedding, prompt_styles, top_k=1):
    from rag_eval import run_full_evaluation

    for style_key in prompt_styles:
        print(f"\nğŸ” ì‹¤í—˜ í”„ë¡¬í”„íŠ¸: {style_key}")
        run_full_evaluation(
            client=client,
            embedding=embedding,
            top_k=top_k,
            prompt_style=style_key
        )

def run_custom_prompt_eval(client, embedding, reranker_path="work1/models/bge-reranker-v2-m3-ko", top_k=1, system_prompt=""):
    from sentence_transformers import CrossEncoder
    from langchain_community.vectorstores import FAISS
    from evaluate import load
    import json
    from tqdm import tqdm

    # ë²¡í„°DB ë° ë¦¬ë­ì»¤ ë¡œë“œ
    reranker = CrossEncoder(reranker_path, device="cuda")
    vectorstore = FAISS.load_local(
        folder_path="faiss_seq",
        embeddings=embedding,
        index_name="index",
        allow_dangerous_deserialization=True
    )

    # ì§ˆë¬¸ ë¡œë“œ
    with open("eval_questions_merged.jsonl", "r", encoding="utf-8") as f:
        eval_questions = [json.loads(line) for line in f]

    f1 = load("evaluate-metric/squad", "f1")
    em = load("evaluate-metric/squad", "exact_match")

    f1_scores, em_scores = [], []

    for idx, qa in tqdm(enumerate(eval_questions), total=len(eval_questions)):
        query = qa["question"]
        answer = qa["answer"]
        qid = qa.get("id", str(idx))

        docs = vectorstore.similarity_search(query, k=top_k)
        reranker_inputs = [[query, doc.page_content] for doc in docs]
        scores = reranker.predict(reranker_inputs)
        reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)

        top_context = "\n\n".join([doc.page_content for doc, _ in reranked[:top_k]])

        user_prompt = f"""ë¬¸ì„œ:
{top_context}

ì§ˆë¬¸: {query}"""

        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0
        )

        prediction = completion.choices[0].message.content.strip()

        prediction_dict = {"id": qid, "prediction_text": prediction}
        reference_dict = {
            "id": qid,
            "answers": [{"text": answer, "answer_start": 0}]
        }

        f1_score = f1.compute(predictions=[prediction_dict], references=[reference_dict])["f1"]
        em_score = em.compute(predictions=[prediction_dict], references=[reference_dict])["exact_match"]

        f1_scores.append(f1_score)
        em_scores.append(em_score)

    print(f"ğŸ“Š í‰ê·  F1: {sum(f1_scores)/len(f1_scores):.2f}")
    print(f"ğŸ“Š í‰ê·  EM: {sum(em_scores)/len(em_scores):.2f}")

def run_custom_prompt_eval_win(client, embedding, eval_path, reranker_path="work1/models/bge-reranker-v2-m3-ko", top_k=1, system_prompt=""):
    from sentence_transformers import CrossEncoder
    from langchain_community.vectorstores import FAISS
    from evaluate import load
    import json
    from tqdm import tqdm

    # ë²¡í„°DB ë° ë¦¬ë­ì»¤ ë¡œë“œ
    reranker = CrossEncoder(reranker_path, device="cuda")
    vectorstore = FAISS.load_local(
        folder_path="faiss_win",
        embeddings=embedding,
        index_name="index",
        allow_dangerous_deserialization=True
    )

    # ğŸ”¹ í‰ê°€ ì§ˆë¬¸ ë¡œë“œ (ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬ëœ ê²½ë¡œ ì‚¬ìš©)
    with open(eval_path, "r", encoding="utf-8") as f:
        eval_questions = [json.loads(line) for line in f]

    f1 = load("evaluate-metric/squad", "f1")
    em = load("evaluate-metric/squad", "exact_match")

    f1_scores, em_scores = [], []

    for idx, qa in tqdm(enumerate(eval_questions), total=len(eval_questions)):
        query = qa["question"]
        answer = qa["answer"]
        qid = qa.get("id", str(idx))

        docs = vectorstore.similarity_search(query, k=top_k)
        reranker_inputs = [[query, doc.page_content] for doc in docs]
        scores = reranker.predict(reranker_inputs)
        reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)

        top_context = "\n\n".join([doc.page_content for doc, _ in reranked[:top_k]])

        user_prompt = f"""ë¬¸ì„œ:
{top_context}

ì§ˆë¬¸: {query}"""

        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0
        )

        prediction = completion.choices[0].message.content.strip()

        prediction_dict = {"id": qid, "prediction_text": prediction}
        reference_dict = {
            "id": qid,
            "answers": [{"text": answer, "answer_start": 0}]
        }

        f1_score = f1.compute(predictions=[prediction_dict], references=[reference_dict])["f1"]
        em_score = em.compute(predictions=[prediction_dict], references=[reference_dict])["exact_match"]

        f1_scores.append(f1_score)
        em_scores.append(em_score)

    print(f"ğŸ“Š í‰ê·  F1: {sum(f1_scores)/len(f1_scores):.2f}")
    print(f"ğŸ“Š í‰ê·  EM: {sum(em_scores)/len(em_scores):.2f}")
