{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필수라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from docx import Document\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"Key\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 및 청킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 전처리 및 청크 완료: 총 52개 저장됨 → output/combined_chunks.json\n"
     ]
    }
   ],
   "source": [
    "### --- 1. 조직도 전처리 --- ###\n",
    "\n",
    "def parse_org_chart_multi_root(paragraphs):\n",
    "    forest = []\n",
    "    stack = []\n",
    "\n",
    "    for line in paragraphs:\n",
    "        indent_level = len(line) - len(line.lstrip('│ '))\n",
    "        clean_line = line.lstrip('│ ').lstrip('├└─ ')\n",
    "        \n",
    "        match = re.match(r'(.+?) — (.+?) \\((.+?)\\)', clean_line)\n",
    "        match_simple = re.match(r'(.+?) \\((.+?)\\)', clean_line)\n",
    "\n",
    "        node = {}\n",
    "        if match:\n",
    "            title, name, position = match.groups()\n",
    "            node = {'type': 'role', 'title': title.strip(), 'name': name.strip(), 'position': position.strip()}\n",
    "        elif match_simple:\n",
    "            name, position = match_simple.groups()\n",
    "            node = {'type': 'person', 'name': name.strip(), 'position': position.strip()}\n",
    "        else:\n",
    "            node = {'type': 'unit', 'name': clean_line.strip()}\n",
    "\n",
    "        node['children'] = []\n",
    "\n",
    "        while stack and stack[-1]['indent'] >= indent_level:\n",
    "            stack.pop()\n",
    "\n",
    "        if stack:\n",
    "            parent = stack[-1]['node']\n",
    "            parent['children'].append(node)\n",
    "        else:\n",
    "            forest.append(node)\n",
    "\n",
    "        stack.append({'indent': indent_level, 'node': node})\n",
    "\n",
    "    return forest\n",
    "\n",
    "def flatten_org_tree_to_chunks(tree, path=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    chunks = []\n",
    "\n",
    "    for node in tree:\n",
    "        current_text = \"\"\n",
    "        metadata = {}\n",
    "\n",
    "        if node[\"type\"] == \"unit\":\n",
    "            current_text = node[\"name\"]\n",
    "        elif node[\"type\"] == \"role\":\n",
    "            current_text = f\"{node['title']} {node['name']} ({node['position']})\"\n",
    "            metadata = {\"title\": node[\"title\"], \"name\": node[\"name\"], \"position\": node[\"position\"]}\n",
    "        elif node[\"type\"] == \"person\":\n",
    "            current_text = f\"{node['name']} ({node['position']})\"\n",
    "            metadata = {\"name\": node[\"name\"], \"position\": node[\"position\"]}\n",
    "\n",
    "        full_path = path + [current_text]\n",
    "        chunk_text = \" > \".join(full_path)\n",
    "\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "        if node.get(\"children\"):\n",
    "            chunks.extend(flatten_org_tree_to_chunks(node[\"children\"], path=full_path))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_org_chart():\n",
    "    docx_path = \"org_chart.docx\"\n",
    "    doc = Document(docx_path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    forest = parse_org_chart_multi_root(paragraphs)\n",
    "    return flatten_org_tree_to_chunks(forest)\n",
    "\n",
    "### --- 2. 인사정보 전처리 --- ###\n",
    "\n",
    "def safe_str(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    return str(val).strip()\n",
    "\n",
    "def process_hr_excel():\n",
    "    xlsx_path = \"HR information.xlsx\"\n",
    "    df = pd.read_excel(xlsx_path, sheet_name=0)\n",
    "    person_chunks = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        name = safe_str(row['성명'])\n",
    "        dept = safe_str(row['부서'])\n",
    "        pos = safe_str(row['직급'])\n",
    "        join = safe_str(row['입사일'])       \n",
    "        duty = safe_str(row['담당 업무'])\n",
    "        eval_ = safe_str(row['최근 평가'])\n",
    "        base = safe_str(row['기본급(₩)'])\n",
    "        bonus = safe_str(row['성과급(₩)'])\n",
    "        cert = safe_str(row['자격증·학위'])\n",
    "        edu = safe_str(row['주요 교육·이수'])\n",
    "        rr = safe_str(row['직무/책임 (R&R)'])\n",
    "\n",
    "        text_parts = [\n",
    "            f\"{name}은(는) {dept} 부서의 {pos}입니다.\",\n",
    "            f\"담당 업무는 {duty}이며, 최근 평가는 {eval_}입니다.\" if duty else \"\",\n",
    "            f\"기본급은 {base}원, 성과급은 {bonus}원입니다.\" if base or bonus else \"\",\n",
    "            f\"보유 자격증 및 학위: {cert}.\" if cert else \"\",\n",
    "            f\"이수 교육: {edu}.\" if edu else \"\",\n",
    "            f\"직무 책임(R&R): {rr}.\" if rr else \"\"\n",
    "        ]\n",
    "        full_text = \" \".join([part for part in text_parts if part])\n",
    "        metadata = {\n",
    "            \"이름\": name,\n",
    "            \"부서\": dept,\n",
    "            \"직급\": pos,\n",
    "            \"입사일\": join,\n",
    "            \"담당업무\": duty,\n",
    "            \"평가\": eval_,\n",
    "            \"기본급\": base,\n",
    "            \"성과급\": bonus,\n",
    "            \"자격증\": cert,\n",
    "            \"교육\": edu,\n",
    "            \"R&R\": rr\n",
    "        }\n",
    "\n",
    "        person_chunks.append({\"text\": full_text, \"metadata\": metadata})\n",
    "    return person_chunks\n",
    "\n",
    "### --- 3. 실행 및 저장 --- ###\n",
    "\n",
    "def run_all():\n",
    "    org_chunks = process_org_chart()\n",
    "    hr_chunks = process_hr_excel()\n",
    "    combined = org_chunks + hr_chunks\n",
    "\n",
    "    # ✅ 저장 경로 고정\n",
    "    save_path = \"output/combined_chunks.json\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ 전처리 및 청크 완료: 총 {len(combined)}개 저장됨 → {save_path}\")\n",
    "\n",
    "### 실행\n",
    "run_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 청크 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 청크 개수: 52개\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"output/combined_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"총 청크 개수: {len(chunks)}개\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 샘플 1 ---\n",
      "📄 TEXT:\n",
      " 김재현은(는) 기술본부 부서의 CTO입니다. 담당 업무는 기술 로드맵·R&D이며, 최근 평가는 A0입니다. 기본급은 8200000원, 성과급은 2000000원입니다. 보유 자격증 및 학위: PhD CS. 이수 교육: AWS Architect. 직무 책임(R&R): 고객 피드백 대응, VOC 수집.\n",
      "📎 METADATA:\n",
      " {'이름': '김재현', '부서': '기술본부', '직급': 'CTO', '입사일': '2019-06-03 00:00:00', '담당업무': '기술 로드맵·R&D', '평가': 'A0', '기본급': '8200000', '성과급': '2000000', '자격증': 'PhD CS', '교육': 'AWS Architect', 'R&R': '고객 피드백 대응, VOC 수집'}\n",
      "\n",
      "--- 샘플 2 ---\n",
      "📄 TEXT:\n",
      " 서준호은(는) 선수관리팀 부서의 대리입니다. 담당 업무는 훈련 동행 및 장비 확인이며, 최근 평가는 B0입니다. 기본급은 3300000원, 성과급은 500000원입니다. 보유 자격증 및 학위: 없음. 이수 교육: 현장 오퍼레이션 트레이닝. 직무 책임(R&R): 장비 체크, 스케줄 현장 대응.\n",
      "📎 METADATA:\n",
      " {'이름': '서준호', '부서': '선수관리팀', '직급': '대리', '입사일': '2024-01-05 00:00:00', '담당업무': '훈련 동행 및 장비 확인', '평가': 'B0', '기본급': '3300000', '성과급': '500000', '자격증': '없음', '교육': '현장 오퍼레이션 트레이닝', 'R&R': '장비 체크, 스케줄 현장 대응'}\n",
      "\n",
      "--- 샘플 3 ---\n",
      "📄 TEXT:\n",
      " CFO — 이재용 > Finance & Accounting 윤태오 (이사)\n",
      "📎 METADATA:\n",
      " {'title': 'Finance & Accounting', 'name': '윤태오', 'position': '이사'}\n",
      "\n",
      "--- 샘플 4 ---\n",
      "📄 TEXT:\n",
      " 정다온은(는) 전략기획팀 부서의 주임입니다. 담당 업무는 경기 리포트 작성이며, 최근 평가는 B0입니다. 기본급은 3600000원, 성과급은 600000원입니다. 보유 자격증 및 학위: 없음. 이수 교육: 전략 분석 및 보고서 작성. 직무 책임(R&R): 경기 분석 보고서, 클라이언트 대응.\n",
      "📎 METADATA:\n",
      " {'이름': '정다온', '부서': '전략기획팀', '직급': '주임', '입사일': '2024-03-10 00:00:00', '담당업무': '경기 리포트 작성', '평가': 'B0', '기본급': '3600000', '성과급': '600000', '자격증': '없음', '교육': '전략 분석 및 보고서 작성', 'R&R': '경기 분석 보고서, 클라이언트 대응'}\n",
      "\n",
      "--- 샘플 5 ---\n",
      "📄 TEXT:\n",
      " CTO — 김재현 > R&D 팀 > 김도윤 (인턴)\n",
      "📎 METADATA:\n",
      " {'name': '김도윤', 'position': '인턴'}\n",
      "\n",
      "--- 샘플 6 ---\n",
      "📄 TEXT:\n",
      " COO — 이석원\n",
      "📎 METADATA:\n",
      " {}\n",
      "\n",
      "--- 샘플 7 ---\n",
      "📄 TEXT:\n",
      " 이석원은(는) 운영본부 부서의 COO입니다. 담당 업무는 운영·프로세스 개선이며, 최근 평가는 A0입니다. 기본급은 7600000원, 성과급은 1600000원입니다. 보유 자격증 및 학위: Six Sigma BB. 이수 교육: Lean Ops. 직무 책임(R&R): 급여, 근태, 복리후생.\n",
      "📎 METADATA:\n",
      " {'이름': '이석원', '부서': '운영본부', '직급': 'COO', '입사일': '2019-09-09 00:00:00', '담당업무': '운영·프로세스 개선', '평가': 'A0', '기본급': '7600000', '성과급': '1600000', '자격증': 'Six Sigma BB', '교육': 'Lean Ops', 'R&R': '급여, 근태, 복리후생'}\n",
      "\n",
      "--- 샘플 8 ---\n",
      "📄 TEXT:\n",
      " 윤태오은(는) 경영지원 부서의 이사입니다. 담당 업무는 회계·세무·예산이며, 최근 평가는 A0입니다. 기본급은 6500000원, 성과급은 1200000원입니다. 보유 자격증 및 학위: 세무사. 이수 교육: 재무관리. 직무 책임(R&R): 테스트 자동화 지원, 스크립트 정리.\n",
      "📎 METADATA:\n",
      " {'이름': '윤태오', '부서': '경영지원', '직급': '이사', '입사일': '2019-04-01 00:00:00', '담당업무': '회계·세무·예산', '평가': 'A0', '기본급': '6500000', '성과급': '1200000', '자격증': '세무사', '교육': '재무관리', 'R&R': '테스트 자동화 지원, 스크립트 정리'}\n",
      "\n",
      "--- 샘플 9 ---\n",
      "📄 TEXT:\n",
      " COO — 이석원 > Customer Support 강민지 (사원)\n",
      "📎 METADATA:\n",
      " {'title': 'Customer Support', 'name': '강민지', 'position': '사원'}\n",
      "\n",
      "--- 샘플 10 ---\n",
      "📄 TEXT:\n",
      " 장서연은(는) 국제업무팀 부서의 대리입니다. 담당 업무는 선수 통역 및 비자 지원이며, 최근 평가는 A+입니다. 기본급은 3850000원, 성과급은 730000원입니다. 보유 자격증 및 학위: 통역사 자격증. 이수 교육: 스포츠 비자/출입국 실무 과정. 직무 책임(R&R): 통역, 출입국 절차, 문서 번역.\n",
      "📎 METADATA:\n",
      " {'이름': '장서연', '부서': '국제업무팀', '직급': '대리', '입사일': '2023-10-12 00:00:00', '담당업무': '선수 통역 및 비자 지원', '평가': 'A+', '기본급': '3850000', '성과급': '730000', '자격증': '통역사 자격증', '교육': '스포츠 비자/출입국 실무 과정', 'R&R': '통역, 출입국 절차, 문서 번역'}\n",
      "\n",
      "--- 샘플 11 ---\n",
      "📄 TEXT:\n",
      " 에이전시사업부 윤권 (사업부장)\n",
      "📎 METADATA:\n",
      " {'title': '에이전시사업부', 'name': '윤권', 'position': '사업부장'}\n",
      "\n",
      "--- 샘플 12 ---\n",
      "📄 TEXT:\n",
      " CTO — 김재현 > R&D 팀 > 정세윤 (주임연구원)\n",
      "📎 METADATA:\n",
      " {'name': '정세윤', 'position': '주임연구원'}\n",
      "\n",
      "--- 샘플 13 ---\n",
      "📄 TEXT:\n",
      " 박지우은(는) 인사총무 부서의 과장입니다. 담당 업무는 인사·급여이며, 최근 평가는 A+입니다. 기본급은 4500000원, 성과급은 800000원입니다. 보유 자격증 및 학위: 노무사. 이수 교육: 노동법 연수. 직무 책임(R&R): 재무계획, 투자 유치, 회계·세무 관리.\n",
      "📎 METADATA:\n",
      " {'이름': '박지우', '부서': '인사총무', '직급': '과장', '입사일': '2020-08-01 00:00:00', '담당업무': '인사·급여', '평가': 'A+', '기본급': '4500000', '성과급': '800000', '자격증': '노무사', '교육': '노동법 연수', 'R&R': '재무계획, 투자 유치, 회계·세무 관리'}\n",
      "\n",
      "--- 샘플 14 ---\n",
      "📄 TEXT:\n",
      " 선수관리팀 > 이하늘 (사원)\n",
      "📎 METADATA:\n",
      " {'name': '이하늘', 'position': '사원'}\n",
      "\n",
      "--- 샘플 15 ---\n",
      "📄 TEXT:\n",
      " HR Director — 임수빈 > HR Assistant 배유진 (대리)\n",
      "📎 METADATA:\n",
      " {'title': 'HR Assistant', 'name': '배유진', 'position': '대리'}\n",
      "\n",
      "--- 샘플 16 ---\n",
      "📄 TEXT:\n",
      " 선수관리팀\n",
      "📎 METADATA:\n",
      " {}\n",
      "\n",
      "--- 샘플 17 ---\n",
      "📄 TEXT:\n",
      " 임수빈은(는) HR실 부서의 HR Director입니다. 담당 업무는 인사전략·DEI이며, 최근 평가는 A0입니다. 기본급은 6200000원, 성과급은 1100000원입니다. 보유 자격증 및 학위: HRD전문가. 이수 교육: DEI Leadership. 직무 책임(R&R): 연차 관리, HR 포털 운영.\n",
      "📎 METADATA:\n",
      " {'이름': '임수빈', '부서': 'HR실', '직급': 'HR Director', '입사일': '2020-05-18 00:00:00', '담당업무': '인사전략·DEI', '평가': 'A0', '기본급': '6200000', '성과급': '1100000', '자격증': 'HRD전문가', '교육': 'DEI Leadership', 'R&R': '연차 관리, HR 포털 운영'}\n",
      "\n",
      "--- 샘플 18 ---\n",
      "📄 TEXT:\n",
      " 선수관리팀 > 김예린 (과장)\n",
      "📎 METADATA:\n",
      " {'name': '김예린', 'position': '과장'}\n",
      "\n",
      "--- 샘플 19 ---\n",
      "📄 TEXT:\n",
      " 박윤서은(는) 선수관리팀 부서의 팀장입니다. 담당 업무는 선수 일정 관리이며, 최근 평가는 A+입니다. 기본급은 3900000원, 성과급은 720000원입니다. 보유 자격증 및 학위: 체육학 학위. 이수 교육: 선수케어 워크숍. 직무 책임(R&R): 출장 스케줄, 이동 동행, 지원.\n",
      "📎 METADATA:\n",
      " {'이름': '박윤서', '부서': '선수관리팀', '직급': '팀장', '입사일': '2024-01-20 00:00:00', '담당업무': '선수 일정 관리', '평가': 'A+', '기본급': '3900000', '성과급': '720000', '자격증': '체육학 학위', '교육': '선수케어 워크숍', 'R&R': '출장 스케줄, 이동 동행, 지원'}\n",
      "\n",
      "--- 샘플 20 ---\n",
      "📄 TEXT:\n",
      " 선수관리팀 > 서준호 (대리)\n",
      "📎 METADATA:\n",
      " {'name': '서준호', 'position': '대리'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i, chunk in enumerate(random.sample(chunks, 20)):\n",
    "    print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "    print(\"📄 TEXT:\\n\", chunk.get(\"text\", \"\"))\n",
    "    print(\"📎 METADATA:\\n\", chunk.get(\"metadata\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 길이: 5자 / 최대 길이: 185자 / 평균: 88자\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(c[\"text\"]) for c in chunks]\n",
    "print(f\"최소 길이: {min(lengths)}자 / 최대 길이: {max(lengths)}자 / 평균: {sum(lengths)//len(lengths)}자\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 메타데이터 키 사용 빈도:\n",
      "  - name: 20개\n",
      "  - position: 20개\n",
      "  - title: 13개\n",
      "  - 이름: 25개\n",
      "  - 부서: 25개\n",
      "  - 직급: 25개\n",
      "  - 입사일: 25개\n",
      "  - 담당업무: 25개\n",
      "  - 평가: 25개\n",
      "  - 기본급: 25개\n",
      "  - 성과급: 25개\n",
      "  - 자격증: 25개\n",
      "  - 교육: 25개\n",
      "  - R&R: 25개\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_metadata(chunks):\n",
    "    all_keys = []\n",
    "    for chunk in chunks:\n",
    "        keys = list(chunk.get(\"metadata\", {}).keys())\n",
    "        all_keys.extend(keys)\n",
    "    return Counter(all_keys)\n",
    "\n",
    "key_stats = analyze_metadata(chunks)\n",
    "print(\"📊 메타데이터 키 사용 빈도:\")\n",
    "for k, v in key_stats.items():\n",
    "    print(f\"  - {k}: {v}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 중복된 청크 수: 0개\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text_counts = Counter(c[\"text\"] for c in chunks)\n",
    "duplicates = [text for text, count in text_counts.items() if count > 1]\n",
    "\n",
    "print(f\"⚠️ 중복된 청크 수: {len(duplicates)}개\")\n",
    "if duplicates:\n",
    "    print(\"중복 예시:\", duplicates[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 '연구' 포함된 청크 수: 5개\n",
      "👉 CTO — 김재현 > R&D 팀 > 김다인 (책임연구원)\n",
      "👉 CTO — 김재현 > R&D 팀 > 정세윤 (주임연구원)\n",
      "👉 김다인은(는) 연구개발 부서의 책임연구원입니다. 담당 업무는 모델 개발·파인튜닝이며, 최근 평가는 A0입니다. 기본급은 5000000원, 성과급은 1000000원입니다. 보유 자격증 및 학위: 정보처리기사. 이수 교육: NLP 심화. 직무 책임(R&R): 기술 로드맵 수립, R&D 관리, 모델 품질 보증.\n"
     ]
    }
   ],
   "source": [
    "keyword = \"연구\"  # 원하는 키워드로 변경 가능\n",
    "filtered = [c for c in chunks if keyword in c[\"text\"]]\n",
    "\n",
    "print(f\"🔎 '{keyword}' 포함된 청크 수: {len(filtered)}개\")\n",
    "for c in filtered[:3]:\n",
    "    print(\"👉\", c[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 ( Vector DB 생성 )\n",
    "1. Faiss DB를 이용\n",
    "2. faiss_org / index.faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdy\\AppData\\Local\\Temp\\ipykernel_25000\\1509701770.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "c:\\Users\\kdy\\anaconda3\\envs\\llm3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS 인덱스 저장 완료 → faiss_org_hr/index.faiss\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1. 임베딩 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"models/KURE-V1\",\n",
    "    model_kwargs={\"device\": \"cuda\"},  # CPU 사용 시 \"cpu\"\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# ✅ 2. combined_chunks.json 불러오기\n",
    "with open(\"output/combined_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# ✅ 3. 문서 포맷으로 정리 (LangChain의 Document 타입 없이 dict로 충분)\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "metadatas = [c.get(\"metadata\", {}) for c in chunks]\n",
    "\n",
    "# ✅ 4. FAISS 벡터 DB 생성\n",
    "vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)\n",
    "\n",
    "# ✅ 5. 저장 경로 지정 및 저장\n",
    "save_dir = \"faiss_org_hr\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "vectorstore.save_local(folder_path=save_dir, index_name=\"index\")\n",
    "\n",
    "print(f\"✅ FAISS 인덱스 저장 완료 → {save_dir}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가지 생성\n",
    "1. OpenAI API를 이용하여 각 항목에 대해1~2개 총 102개의 질문지 생성\n",
    "2. gpt-4o 모델을 이용하여 정확한 평가지 작성성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 0/52개 처리 중...\n",
      "🔄 10/52개 처리 중...\n",
      "🔄 20/52개 처리 중...\n",
      "🔄 30/52개 처리 중...\n",
      "🔄 40/52개 처리 중...\n",
      "🔄 50/52개 처리 중...\n",
      "✅ GPT 기반 질문지 102개 저장 완료 → output/eval_questions_gpt.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 평가지 만들기 \n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ✅ OpenAI client (환경변수로 키 관리 중)\n",
    "client = OpenAI()\n",
    "\n",
    "# ✅ 1. 청크 불러오기\n",
    "with open(\"output/combined_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# ✅ 2. 질문 생성 프롬프트 정의\n",
    "def build_prompt(text):\n",
    "    return f\"\"\"다음 텍스트에서 의미 기반으로 질의응답 쌍을 1~2개 만들어주세요.\n",
    "\n",
    "텍스트:\n",
    "{text}\n",
    "\n",
    "형식: \n",
    "[\n",
    "  {{\"question\": \"질문 내용\", \"answer\": \"정답 내용\"}}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# ✅ 3. 질문 생성 함수 (GPT 호출)\n",
    "def generate_qa_from_text(text):\n",
    "    prompt = build_prompt(text)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 평가용 질문지를 만드는 어시스턴트입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        qa_pairs = json.loads(content)\n",
    "        return qa_pairs\n",
    "    except Exception as e:\n",
    "        print(\"❌ 오류 발생:\", e)\n",
    "        return []\n",
    "\n",
    "# ✅ 4. 전체 QA 생성 실행\n",
    "qa_dataset = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    text = chunk[\"text\"]\n",
    "    qa_pairs = generate_qa_from_text(text)\n",
    "    for qa in qa_pairs:\n",
    "        if qa.get(\"question\") and qa.get(\"answer\"):\n",
    "            qa_dataset.append(qa)\n",
    "    time.sleep(0.5)  # 💡 rate limit 방지 (필요시 조정)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"🔄 {i}/{len(chunks)}개 처리 중...\")\n",
    "\n",
    "# ✅ 5. 저장\n",
    "save_path = \"output/eval_questions_gpt.jsonl\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_dataset:\n",
    "        f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ GPT 기반 질문지 {len(qa_dataset)}개 저장 완료 → {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm3)",
   "language": "python",
   "name": "llm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
