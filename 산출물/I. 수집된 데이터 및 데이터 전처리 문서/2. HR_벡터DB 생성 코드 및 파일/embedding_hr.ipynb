{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•„ìˆ˜ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from docx import Document\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"Key\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ ë° ì²­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²˜ë¦¬ ë° ì²­í¬ ì™„ë£Œ: ì´ 52ê°œ ì €ì¥ë¨ â†’ output/combined_chunks.json\n"
     ]
    }
   ],
   "source": [
    "### --- 1. ì¡°ì§ë„ ì „ì²˜ë¦¬ --- ###\n",
    "\n",
    "def parse_org_chart_multi_root(paragraphs):\n",
    "    forest = []\n",
    "    stack = []\n",
    "\n",
    "    for line in paragraphs:\n",
    "        indent_level = len(line) - len(line.lstrip('â”‚ '))\n",
    "        clean_line = line.lstrip('â”‚ ').lstrip('â”œâ””â”€ ')\n",
    "        \n",
    "        match = re.match(r'(.+?) â€” (.+?) \\((.+?)\\)', clean_line)\n",
    "        match_simple = re.match(r'(.+?) \\((.+?)\\)', clean_line)\n",
    "\n",
    "        node = {}\n",
    "        if match:\n",
    "            title, name, position = match.groups()\n",
    "            node = {'type': 'role', 'title': title.strip(), 'name': name.strip(), 'position': position.strip()}\n",
    "        elif match_simple:\n",
    "            name, position = match_simple.groups()\n",
    "            node = {'type': 'person', 'name': name.strip(), 'position': position.strip()}\n",
    "        else:\n",
    "            node = {'type': 'unit', 'name': clean_line.strip()}\n",
    "\n",
    "        node['children'] = []\n",
    "\n",
    "        while stack and stack[-1]['indent'] >= indent_level:\n",
    "            stack.pop()\n",
    "\n",
    "        if stack:\n",
    "            parent = stack[-1]['node']\n",
    "            parent['children'].append(node)\n",
    "        else:\n",
    "            forest.append(node)\n",
    "\n",
    "        stack.append({'indent': indent_level, 'node': node})\n",
    "\n",
    "    return forest\n",
    "\n",
    "def flatten_org_tree_to_chunks(tree, path=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    chunks = []\n",
    "\n",
    "    for node in tree:\n",
    "        current_text = \"\"\n",
    "        metadata = {}\n",
    "\n",
    "        if node[\"type\"] == \"unit\":\n",
    "            current_text = node[\"name\"]\n",
    "        elif node[\"type\"] == \"role\":\n",
    "            current_text = f\"{node['title']} {node['name']} ({node['position']})\"\n",
    "            metadata = {\"title\": node[\"title\"], \"name\": node[\"name\"], \"position\": node[\"position\"]}\n",
    "        elif node[\"type\"] == \"person\":\n",
    "            current_text = f\"{node['name']} ({node['position']})\"\n",
    "            metadata = {\"name\": node[\"name\"], \"position\": node[\"position\"]}\n",
    "\n",
    "        full_path = path + [current_text]\n",
    "        chunk_text = \" > \".join(full_path)\n",
    "\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "        if node.get(\"children\"):\n",
    "            chunks.extend(flatten_org_tree_to_chunks(node[\"children\"], path=full_path))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_org_chart():\n",
    "    docx_path = \"org_chart.docx\"\n",
    "    doc = Document(docx_path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    forest = parse_org_chart_multi_root(paragraphs)\n",
    "    return flatten_org_tree_to_chunks(forest)\n",
    "\n",
    "### --- 2. ì¸ì‚¬ì •ë³´ ì „ì²˜ë¦¬ --- ###\n",
    "\n",
    "def safe_str(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    return str(val).strip()\n",
    "\n",
    "def process_hr_excel():\n",
    "    xlsx_path = \"HR information.xlsx\"\n",
    "    df = pd.read_excel(xlsx_path, sheet_name=0)\n",
    "    person_chunks = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        name = safe_str(row['ì„±ëª…'])\n",
    "        dept = safe_str(row['ë¶€ì„œ'])\n",
    "        pos = safe_str(row['ì§ê¸‰'])\n",
    "        join = safe_str(row['ì…ì‚¬ì¼'])       \n",
    "        duty = safe_str(row['ë‹´ë‹¹ ì—…ë¬´'])\n",
    "        eval_ = safe_str(row['ìµœê·¼ í‰ê°€'])\n",
    "        base = safe_str(row['ê¸°ë³¸ê¸‰(â‚©)'])\n",
    "        bonus = safe_str(row['ì„±ê³¼ê¸‰(â‚©)'])\n",
    "        cert = safe_str(row['ìê²©ì¦Â·í•™ìœ„'])\n",
    "        edu = safe_str(row['ì£¼ìš” êµìœ¡Â·ì´ìˆ˜'])\n",
    "        rr = safe_str(row['ì§ë¬´/ì±…ì„ (R&R)'])\n",
    "\n",
    "        text_parts = [\n",
    "            f\"{name}ì€(ëŠ”) {dept} ë¶€ì„œì˜ {pos}ì…ë‹ˆë‹¤.\",\n",
    "            f\"ë‹´ë‹¹ ì—…ë¬´ëŠ” {duty}ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” {eval_}ì…ë‹ˆë‹¤.\" if duty else \"\",\n",
    "            f\"ê¸°ë³¸ê¸‰ì€ {base}ì›, ì„±ê³¼ê¸‰ì€ {bonus}ì›ì…ë‹ˆë‹¤.\" if base or bonus else \"\",\n",
    "            f\"ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: {cert}.\" if cert else \"\",\n",
    "            f\"ì´ìˆ˜ êµìœ¡: {edu}.\" if edu else \"\",\n",
    "            f\"ì§ë¬´ ì±…ì„(R&R): {rr}.\" if rr else \"\"\n",
    "        ]\n",
    "        full_text = \" \".join([part for part in text_parts if part])\n",
    "        metadata = {\n",
    "            \"ì´ë¦„\": name,\n",
    "            \"ë¶€ì„œ\": dept,\n",
    "            \"ì§ê¸‰\": pos,\n",
    "            \"ì…ì‚¬ì¼\": join,\n",
    "            \"ë‹´ë‹¹ì—…ë¬´\": duty,\n",
    "            \"í‰ê°€\": eval_,\n",
    "            \"ê¸°ë³¸ê¸‰\": base,\n",
    "            \"ì„±ê³¼ê¸‰\": bonus,\n",
    "            \"ìê²©ì¦\": cert,\n",
    "            \"êµìœ¡\": edu,\n",
    "            \"R&R\": rr\n",
    "        }\n",
    "\n",
    "        person_chunks.append({\"text\": full_text, \"metadata\": metadata})\n",
    "    return person_chunks\n",
    "\n",
    "### --- 3. ì‹¤í–‰ ë° ì €ì¥ --- ###\n",
    "\n",
    "def run_all():\n",
    "    org_chunks = process_org_chart()\n",
    "    hr_chunks = process_hr_excel()\n",
    "    combined = org_chunks + hr_chunks\n",
    "\n",
    "    # âœ… ì €ì¥ ê²½ë¡œ ê³ ì •\n",
    "    save_path = \"output/combined_chunks.json\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… ì „ì²˜ë¦¬ ë° ì²­í¬ ì™„ë£Œ: ì´ {len(combined)}ê°œ ì €ì¥ë¨ â†’ {save_path}\")\n",
    "\n",
    "### ì‹¤í–‰\n",
    "run_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì²­í¬ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì²­í¬ ê°œìˆ˜: 52ê°œ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"output/combined_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"ì´ ì²­í¬ ê°œìˆ˜: {len(chunks)}ê°œ\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ìƒ˜í”Œ 1 ---\n",
      "ğŸ“„ TEXT:\n",
      " ê¹€ì¬í˜„ì€(ëŠ”) ê¸°ìˆ ë³¸ë¶€ ë¶€ì„œì˜ CTOì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ê¸°ìˆ  ë¡œë“œë§µÂ·R&Dì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 8200000ì›, ì„±ê³¼ê¸‰ì€ 2000000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: PhD CS. ì´ìˆ˜ êµìœ¡: AWS Architect. ì§ë¬´ ì±…ì„(R&R): ê³ ê° í”¼ë“œë°± ëŒ€ì‘, VOC ìˆ˜ì§‘.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ê¹€ì¬í˜„', 'ë¶€ì„œ': 'ê¸°ìˆ ë³¸ë¶€', 'ì§ê¸‰': 'CTO', 'ì…ì‚¬ì¼': '2019-06-03 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ê¸°ìˆ  ë¡œë“œë§µÂ·R&D', 'í‰ê°€': 'A0', 'ê¸°ë³¸ê¸‰': '8200000', 'ì„±ê³¼ê¸‰': '2000000', 'ìê²©ì¦': 'PhD CS', 'êµìœ¡': 'AWS Architect', 'R&R': 'ê³ ê° í”¼ë“œë°± ëŒ€ì‘, VOC ìˆ˜ì§‘'}\n",
      "\n",
      "--- ìƒ˜í”Œ 2 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì„œì¤€í˜¸ì€(ëŠ”) ì„ ìˆ˜ê´€ë¦¬íŒ€ ë¶€ì„œì˜ ëŒ€ë¦¬ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” í›ˆë ¨ ë™í–‰ ë° ì¥ë¹„ í™•ì¸ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” B0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 3300000ì›, ì„±ê³¼ê¸‰ì€ 500000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: ì—†ìŒ. ì´ìˆ˜ êµìœ¡: í˜„ì¥ ì˜¤í¼ë ˆì´ì…˜ íŠ¸ë ˆì´ë‹. ì§ë¬´ ì±…ì„(R&R): ì¥ë¹„ ì²´í¬, ìŠ¤ì¼€ì¤„ í˜„ì¥ ëŒ€ì‘.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ì„œì¤€í˜¸', 'ë¶€ì„œ': 'ì„ ìˆ˜ê´€ë¦¬íŒ€', 'ì§ê¸‰': 'ëŒ€ë¦¬', 'ì…ì‚¬ì¼': '2024-01-05 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'í›ˆë ¨ ë™í–‰ ë° ì¥ë¹„ í™•ì¸', 'í‰ê°€': 'B0', 'ê¸°ë³¸ê¸‰': '3300000', 'ì„±ê³¼ê¸‰': '500000', 'ìê²©ì¦': 'ì—†ìŒ', 'êµìœ¡': 'í˜„ì¥ ì˜¤í¼ë ˆì´ì…˜ íŠ¸ë ˆì´ë‹', 'R&R': 'ì¥ë¹„ ì²´í¬, ìŠ¤ì¼€ì¤„ í˜„ì¥ ëŒ€ì‘'}\n",
      "\n",
      "--- ìƒ˜í”Œ 3 ---\n",
      "ğŸ“„ TEXT:\n",
      " CFO â€” ì´ì¬ìš© > Finance & Accounting ìœ¤íƒœì˜¤ (ì´ì‚¬)\n",
      "ğŸ“ METADATA:\n",
      " {'title': 'Finance & Accounting', 'name': 'ìœ¤íƒœì˜¤', 'position': 'ì´ì‚¬'}\n",
      "\n",
      "--- ìƒ˜í”Œ 4 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì •ë‹¤ì˜¨ì€(ëŠ”) ì „ëµê¸°íšíŒ€ ë¶€ì„œì˜ ì£¼ì„ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ê²½ê¸° ë¦¬í¬íŠ¸ ì‘ì„±ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” B0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 3600000ì›, ì„±ê³¼ê¸‰ì€ 600000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: ì—†ìŒ. ì´ìˆ˜ êµìœ¡: ì „ëµ ë¶„ì„ ë° ë³´ê³ ì„œ ì‘ì„±. ì§ë¬´ ì±…ì„(R&R): ê²½ê¸° ë¶„ì„ ë³´ê³ ì„œ, í´ë¼ì´ì–¸íŠ¸ ëŒ€ì‘.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ì •ë‹¤ì˜¨', 'ë¶€ì„œ': 'ì „ëµê¸°íšíŒ€', 'ì§ê¸‰': 'ì£¼ì„', 'ì…ì‚¬ì¼': '2024-03-10 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ê²½ê¸° ë¦¬í¬íŠ¸ ì‘ì„±', 'í‰ê°€': 'B0', 'ê¸°ë³¸ê¸‰': '3600000', 'ì„±ê³¼ê¸‰': '600000', 'ìê²©ì¦': 'ì—†ìŒ', 'êµìœ¡': 'ì „ëµ ë¶„ì„ ë° ë³´ê³ ì„œ ì‘ì„±', 'R&R': 'ê²½ê¸° ë¶„ì„ ë³´ê³ ì„œ, í´ë¼ì´ì–¸íŠ¸ ëŒ€ì‘'}\n",
      "\n",
      "--- ìƒ˜í”Œ 5 ---\n",
      "ğŸ“„ TEXT:\n",
      " CTO â€” ê¹€ì¬í˜„ > R&D íŒ€ > ê¹€ë„ìœ¤ (ì¸í„´)\n",
      "ğŸ“ METADATA:\n",
      " {'name': 'ê¹€ë„ìœ¤', 'position': 'ì¸í„´'}\n",
      "\n",
      "--- ìƒ˜í”Œ 6 ---\n",
      "ğŸ“„ TEXT:\n",
      " COO â€” ì´ì„ì›\n",
      "ğŸ“ METADATA:\n",
      " {}\n",
      "\n",
      "--- ìƒ˜í”Œ 7 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì´ì„ì›ì€(ëŠ”) ìš´ì˜ë³¸ë¶€ ë¶€ì„œì˜ COOì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ìš´ì˜Â·í”„ë¡œì„¸ìŠ¤ ê°œì„ ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 7600000ì›, ì„±ê³¼ê¸‰ì€ 1600000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: Six Sigma BB. ì´ìˆ˜ êµìœ¡: Lean Ops. ì§ë¬´ ì±…ì„(R&R): ê¸‰ì—¬, ê·¼íƒœ, ë³µë¦¬í›„ìƒ.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ì´ì„ì›', 'ë¶€ì„œ': 'ìš´ì˜ë³¸ë¶€', 'ì§ê¸‰': 'COO', 'ì…ì‚¬ì¼': '2019-09-09 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ìš´ì˜Â·í”„ë¡œì„¸ìŠ¤ ê°œì„ ', 'í‰ê°€': 'A0', 'ê¸°ë³¸ê¸‰': '7600000', 'ì„±ê³¼ê¸‰': '1600000', 'ìê²©ì¦': 'Six Sigma BB', 'êµìœ¡': 'Lean Ops', 'R&R': 'ê¸‰ì—¬, ê·¼íƒœ, ë³µë¦¬í›„ìƒ'}\n",
      "\n",
      "--- ìƒ˜í”Œ 8 ---\n",
      "ğŸ“„ TEXT:\n",
      " ìœ¤íƒœì˜¤ì€(ëŠ”) ê²½ì˜ì§€ì› ë¶€ì„œì˜ ì´ì‚¬ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” íšŒê³„Â·ì„¸ë¬´Â·ì˜ˆì‚°ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 6500000ì›, ì„±ê³¼ê¸‰ì€ 1200000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: ì„¸ë¬´ì‚¬. ì´ìˆ˜ êµìœ¡: ì¬ë¬´ê´€ë¦¬. ì§ë¬´ ì±…ì„(R&R): í…ŒìŠ¤íŠ¸ ìë™í™” ì§€ì›, ìŠ¤í¬ë¦½íŠ¸ ì •ë¦¬.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ìœ¤íƒœì˜¤', 'ë¶€ì„œ': 'ê²½ì˜ì§€ì›', 'ì§ê¸‰': 'ì´ì‚¬', 'ì…ì‚¬ì¼': '2019-04-01 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'íšŒê³„Â·ì„¸ë¬´Â·ì˜ˆì‚°', 'í‰ê°€': 'A0', 'ê¸°ë³¸ê¸‰': '6500000', 'ì„±ê³¼ê¸‰': '1200000', 'ìê²©ì¦': 'ì„¸ë¬´ì‚¬', 'êµìœ¡': 'ì¬ë¬´ê´€ë¦¬', 'R&R': 'í…ŒìŠ¤íŠ¸ ìë™í™” ì§€ì›, ìŠ¤í¬ë¦½íŠ¸ ì •ë¦¬'}\n",
      "\n",
      "--- ìƒ˜í”Œ 9 ---\n",
      "ğŸ“„ TEXT:\n",
      " COO â€” ì´ì„ì› > Customer Support ê°•ë¯¼ì§€ (ì‚¬ì›)\n",
      "ğŸ“ METADATA:\n",
      " {'title': 'Customer Support', 'name': 'ê°•ë¯¼ì§€', 'position': 'ì‚¬ì›'}\n",
      "\n",
      "--- ìƒ˜í”Œ 10 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì¥ì„œì—°ì€(ëŠ”) êµ­ì œì—…ë¬´íŒ€ ë¶€ì„œì˜ ëŒ€ë¦¬ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ì„ ìˆ˜ í†µì—­ ë° ë¹„ì ì§€ì›ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A+ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 3850000ì›, ì„±ê³¼ê¸‰ì€ 730000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: í†µì—­ì‚¬ ìê²©ì¦. ì´ìˆ˜ êµìœ¡: ìŠ¤í¬ì¸  ë¹„ì/ì¶œì…êµ­ ì‹¤ë¬´ ê³¼ì •. ì§ë¬´ ì±…ì„(R&R): í†µì—­, ì¶œì…êµ­ ì ˆì°¨, ë¬¸ì„œ ë²ˆì—­.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ì¥ì„œì—°', 'ë¶€ì„œ': 'êµ­ì œì—…ë¬´íŒ€', 'ì§ê¸‰': 'ëŒ€ë¦¬', 'ì…ì‚¬ì¼': '2023-10-12 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ì„ ìˆ˜ í†µì—­ ë° ë¹„ì ì§€ì›', 'í‰ê°€': 'A+', 'ê¸°ë³¸ê¸‰': '3850000', 'ì„±ê³¼ê¸‰': '730000', 'ìê²©ì¦': 'í†µì—­ì‚¬ ìê²©ì¦', 'êµìœ¡': 'ìŠ¤í¬ì¸  ë¹„ì/ì¶œì…êµ­ ì‹¤ë¬´ ê³¼ì •', 'R&R': 'í†µì—­, ì¶œì…êµ­ ì ˆì°¨, ë¬¸ì„œ ë²ˆì—­'}\n",
      "\n",
      "--- ìƒ˜í”Œ 11 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì—ì´ì „ì‹œì‚¬ì—…ë¶€ ìœ¤ê¶Œ (ì‚¬ì—…ë¶€ì¥)\n",
      "ğŸ“ METADATA:\n",
      " {'title': 'ì—ì´ì „ì‹œì‚¬ì—…ë¶€', 'name': 'ìœ¤ê¶Œ', 'position': 'ì‚¬ì—…ë¶€ì¥'}\n",
      "\n",
      "--- ìƒ˜í”Œ 12 ---\n",
      "ğŸ“„ TEXT:\n",
      " CTO â€” ê¹€ì¬í˜„ > R&D íŒ€ > ì •ì„¸ìœ¤ (ì£¼ì„ì—°êµ¬ì›)\n",
      "ğŸ“ METADATA:\n",
      " {'name': 'ì •ì„¸ìœ¤', 'position': 'ì£¼ì„ì—°êµ¬ì›'}\n",
      "\n",
      "--- ìƒ˜í”Œ 13 ---\n",
      "ğŸ“„ TEXT:\n",
      " ë°•ì§€ìš°ì€(ëŠ”) ì¸ì‚¬ì´ë¬´ ë¶€ì„œì˜ ê³¼ì¥ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ì¸ì‚¬Â·ê¸‰ì—¬ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A+ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 4500000ì›, ì„±ê³¼ê¸‰ì€ 800000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: ë…¸ë¬´ì‚¬. ì´ìˆ˜ êµìœ¡: ë…¸ë™ë²• ì—°ìˆ˜. ì§ë¬´ ì±…ì„(R&R): ì¬ë¬´ê³„íš, íˆ¬ì ìœ ì¹˜, íšŒê³„Â·ì„¸ë¬´ ê´€ë¦¬.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ë°•ì§€ìš°', 'ë¶€ì„œ': 'ì¸ì‚¬ì´ë¬´', 'ì§ê¸‰': 'ê³¼ì¥', 'ì…ì‚¬ì¼': '2020-08-01 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ì¸ì‚¬Â·ê¸‰ì—¬', 'í‰ê°€': 'A+', 'ê¸°ë³¸ê¸‰': '4500000', 'ì„±ê³¼ê¸‰': '800000', 'ìê²©ì¦': 'ë…¸ë¬´ì‚¬', 'êµìœ¡': 'ë…¸ë™ë²• ì—°ìˆ˜', 'R&R': 'ì¬ë¬´ê³„íš, íˆ¬ì ìœ ì¹˜, íšŒê³„Â·ì„¸ë¬´ ê´€ë¦¬'}\n",
      "\n",
      "--- ìƒ˜í”Œ 14 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì„ ìˆ˜ê´€ë¦¬íŒ€ > ì´í•˜ëŠ˜ (ì‚¬ì›)\n",
      "ğŸ“ METADATA:\n",
      " {'name': 'ì´í•˜ëŠ˜', 'position': 'ì‚¬ì›'}\n",
      "\n",
      "--- ìƒ˜í”Œ 15 ---\n",
      "ğŸ“„ TEXT:\n",
      " HR Director â€” ì„ìˆ˜ë¹ˆ > HR Assistant ë°°ìœ ì§„ (ëŒ€ë¦¬)\n",
      "ğŸ“ METADATA:\n",
      " {'title': 'HR Assistant', 'name': 'ë°°ìœ ì§„', 'position': 'ëŒ€ë¦¬'}\n",
      "\n",
      "--- ìƒ˜í”Œ 16 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì„ ìˆ˜ê´€ë¦¬íŒ€\n",
      "ğŸ“ METADATA:\n",
      " {}\n",
      "\n",
      "--- ìƒ˜í”Œ 17 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì„ìˆ˜ë¹ˆì€(ëŠ”) HRì‹¤ ë¶€ì„œì˜ HR Directorì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ì¸ì‚¬ì „ëµÂ·DEIì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 6200000ì›, ì„±ê³¼ê¸‰ì€ 1100000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: HRDì „ë¬¸ê°€. ì´ìˆ˜ êµìœ¡: DEI Leadership. ì§ë¬´ ì±…ì„(R&R): ì—°ì°¨ ê´€ë¦¬, HR í¬í„¸ ìš´ì˜.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ì„ìˆ˜ë¹ˆ', 'ë¶€ì„œ': 'HRì‹¤', 'ì§ê¸‰': 'HR Director', 'ì…ì‚¬ì¼': '2020-05-18 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ì¸ì‚¬ì „ëµÂ·DEI', 'í‰ê°€': 'A0', 'ê¸°ë³¸ê¸‰': '6200000', 'ì„±ê³¼ê¸‰': '1100000', 'ìê²©ì¦': 'HRDì „ë¬¸ê°€', 'êµìœ¡': 'DEI Leadership', 'R&R': 'ì—°ì°¨ ê´€ë¦¬, HR í¬í„¸ ìš´ì˜'}\n",
      "\n",
      "--- ìƒ˜í”Œ 18 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì„ ìˆ˜ê´€ë¦¬íŒ€ > ê¹€ì˜ˆë¦° (ê³¼ì¥)\n",
      "ğŸ“ METADATA:\n",
      " {'name': 'ê¹€ì˜ˆë¦°', 'position': 'ê³¼ì¥'}\n",
      "\n",
      "--- ìƒ˜í”Œ 19 ---\n",
      "ğŸ“„ TEXT:\n",
      " ë°•ìœ¤ì„œì€(ëŠ”) ì„ ìˆ˜ê´€ë¦¬íŒ€ ë¶€ì„œì˜ íŒ€ì¥ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ì„ ìˆ˜ ì¼ì • ê´€ë¦¬ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A+ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 3900000ì›, ì„±ê³¼ê¸‰ì€ 720000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: ì²´ìœ¡í•™ í•™ìœ„. ì´ìˆ˜ êµìœ¡: ì„ ìˆ˜ì¼€ì–´ ì›Œí¬ìˆ. ì§ë¬´ ì±…ì„(R&R): ì¶œì¥ ìŠ¤ì¼€ì¤„, ì´ë™ ë™í–‰, ì§€ì›.\n",
      "ğŸ“ METADATA:\n",
      " {'ì´ë¦„': 'ë°•ìœ¤ì„œ', 'ë¶€ì„œ': 'ì„ ìˆ˜ê´€ë¦¬íŒ€', 'ì§ê¸‰': 'íŒ€ì¥', 'ì…ì‚¬ì¼': '2024-01-20 00:00:00', 'ë‹´ë‹¹ì—…ë¬´': 'ì„ ìˆ˜ ì¼ì • ê´€ë¦¬', 'í‰ê°€': 'A+', 'ê¸°ë³¸ê¸‰': '3900000', 'ì„±ê³¼ê¸‰': '720000', 'ìê²©ì¦': 'ì²´ìœ¡í•™ í•™ìœ„', 'êµìœ¡': 'ì„ ìˆ˜ì¼€ì–´ ì›Œí¬ìˆ', 'R&R': 'ì¶œì¥ ìŠ¤ì¼€ì¤„, ì´ë™ ë™í–‰, ì§€ì›'}\n",
      "\n",
      "--- ìƒ˜í”Œ 20 ---\n",
      "ğŸ“„ TEXT:\n",
      " ì„ ìˆ˜ê´€ë¦¬íŒ€ > ì„œì¤€í˜¸ (ëŒ€ë¦¬)\n",
      "ğŸ“ METADATA:\n",
      " {'name': 'ì„œì¤€í˜¸', 'position': 'ëŒ€ë¦¬'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i, chunk in enumerate(random.sample(chunks, 20)):\n",
    "    print(f\"\\n--- ìƒ˜í”Œ {i+1} ---\")\n",
    "    print(\"ğŸ“„ TEXT:\\n\", chunk.get(\"text\", \"\"))\n",
    "    print(\"ğŸ“ METADATA:\\n\", chunk.get(\"metadata\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì†Œ ê¸¸ì´: 5ì / ìµœëŒ€ ê¸¸ì´: 185ì / í‰ê· : 88ì\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(c[\"text\"]) for c in chunks]\n",
    "print(f\"ìµœì†Œ ê¸¸ì´: {min(lengths)}ì / ìµœëŒ€ ê¸¸ì´: {max(lengths)}ì / í‰ê· : {sum(lengths)//len(lengths)}ì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë©”íƒ€ë°ì´í„° í‚¤ ì‚¬ìš© ë¹ˆë„:\n",
      "  - name: 20ê°œ\n",
      "  - position: 20ê°œ\n",
      "  - title: 13ê°œ\n",
      "  - ì´ë¦„: 25ê°œ\n",
      "  - ë¶€ì„œ: 25ê°œ\n",
      "  - ì§ê¸‰: 25ê°œ\n",
      "  - ì…ì‚¬ì¼: 25ê°œ\n",
      "  - ë‹´ë‹¹ì—…ë¬´: 25ê°œ\n",
      "  - í‰ê°€: 25ê°œ\n",
      "  - ê¸°ë³¸ê¸‰: 25ê°œ\n",
      "  - ì„±ê³¼ê¸‰: 25ê°œ\n",
      "  - ìê²©ì¦: 25ê°œ\n",
      "  - êµìœ¡: 25ê°œ\n",
      "  - R&R: 25ê°œ\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_metadata(chunks):\n",
    "    all_keys = []\n",
    "    for chunk in chunks:\n",
    "        keys = list(chunk.get(\"metadata\", {}).keys())\n",
    "        all_keys.extend(keys)\n",
    "    return Counter(all_keys)\n",
    "\n",
    "key_stats = analyze_metadata(chunks)\n",
    "print(\"ğŸ“Š ë©”íƒ€ë°ì´í„° í‚¤ ì‚¬ìš© ë¹ˆë„:\")\n",
    "for k, v in key_stats.items():\n",
    "    print(f\"  - {k}: {v}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì¤‘ë³µëœ ì²­í¬ ìˆ˜: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text_counts = Counter(c[\"text\"] for c in chunks)\n",
    "duplicates = [text for text, count in text_counts.items() if count > 1]\n",
    "\n",
    "print(f\"âš ï¸ ì¤‘ë³µëœ ì²­í¬ ìˆ˜: {len(duplicates)}ê°œ\")\n",
    "if duplicates:\n",
    "    print(\"ì¤‘ë³µ ì˜ˆì‹œ:\", duplicates[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 'ì—°êµ¬' í¬í•¨ëœ ì²­í¬ ìˆ˜: 5ê°œ\n",
      "ğŸ‘‰ CTO â€” ê¹€ì¬í˜„ > R&D íŒ€ > ê¹€ë‹¤ì¸ (ì±…ì„ì—°êµ¬ì›)\n",
      "ğŸ‘‰ CTO â€” ê¹€ì¬í˜„ > R&D íŒ€ > ì •ì„¸ìœ¤ (ì£¼ì„ì—°êµ¬ì›)\n",
      "ğŸ‘‰ ê¹€ë‹¤ì¸ì€(ëŠ”) ì—°êµ¬ê°œë°œ ë¶€ì„œì˜ ì±…ì„ì—°êµ¬ì›ì…ë‹ˆë‹¤. ë‹´ë‹¹ ì—…ë¬´ëŠ” ëª¨ë¸ ê°œë°œÂ·íŒŒì¸íŠœë‹ì´ë©°, ìµœê·¼ í‰ê°€ëŠ” A0ì…ë‹ˆë‹¤. ê¸°ë³¸ê¸‰ì€ 5000000ì›, ì„±ê³¼ê¸‰ì€ 1000000ì›ì…ë‹ˆë‹¤. ë³´ìœ  ìê²©ì¦ ë° í•™ìœ„: ì •ë³´ì²˜ë¦¬ê¸°ì‚¬. ì´ìˆ˜ êµìœ¡: NLP ì‹¬í™”. ì§ë¬´ ì±…ì„(R&R): ê¸°ìˆ  ë¡œë“œë§µ ìˆ˜ë¦½, R&D ê´€ë¦¬, ëª¨ë¸ í’ˆì§ˆ ë³´ì¦.\n"
     ]
    }
   ],
   "source": [
    "keyword = \"ì—°êµ¬\"  # ì›í•˜ëŠ” í‚¤ì›Œë“œë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "filtered = [c for c in chunks if keyword in c[\"text\"]]\n",
    "\n",
    "print(f\"ğŸ” '{keyword}' í¬í•¨ëœ ì²­í¬ ìˆ˜: {len(filtered)}ê°œ\")\n",
    "for c in filtered[:3]:\n",
    "    print(\"ğŸ‘‰\", c[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì„ë² ë”© ( Vector DB ìƒì„± )\n",
    "1. Faiss DBë¥¼ ì´ìš©\n",
    "2. faiss_org / index.faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdy\\AppData\\Local\\Temp\\ipykernel_25000\\1509701770.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "c:\\Users\\kdy\\anaconda3\\envs\\llm3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ faiss_org_hr/index.faiss\n"
     ]
    }
   ],
   "source": [
    "# âœ… 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"models/KURE-V1\",\n",
    "    model_kwargs={\"device\": \"cuda\"},  # CPU ì‚¬ìš© ì‹œ \"cpu\"\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# âœ… 2. combined_chunks.json ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"output/combined_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# âœ… 3. ë¬¸ì„œ í¬ë§·ìœ¼ë¡œ ì •ë¦¬ (LangChainì˜ Document íƒ€ì… ì—†ì´ dictë¡œ ì¶©ë¶„)\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "metadatas = [c.get(\"metadata\", {}) for c in chunks]\n",
    "\n",
    "# âœ… 4. FAISS ë²¡í„° DB ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)\n",
    "\n",
    "# âœ… 5. ì €ì¥ ê²½ë¡œ ì§€ì • ë° ì €ì¥\n",
    "save_dir = \"faiss_org_hr\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "vectorstore.save_local(folder_path=save_dir, index_name=\"index\")\n",
    "\n",
    "print(f\"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {save_dir}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í‰ê°€ì§€ ìƒì„±\n",
    "1. OpenAI APIë¥¼ ì´ìš©í•˜ì—¬ ê° í•­ëª©ì— ëŒ€í•´1~2ê°œ ì´ 102ê°œì˜ ì§ˆë¬¸ì§€ ìƒì„±\n",
    "2. gpt-4o ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì •í™•í•œ í‰ê°€ì§€ ì‘ì„±ì„± "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ 0/52ê°œ ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ”„ 10/52ê°œ ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ”„ 20/52ê°œ ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ”„ 30/52ê°œ ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ”„ 40/52ê°œ ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ”„ 50/52ê°œ ì²˜ë¦¬ ì¤‘...\n",
      "âœ… GPT ê¸°ë°˜ ì§ˆë¬¸ì§€ 102ê°œ ì €ì¥ ì™„ë£Œ â†’ output/eval_questions_gpt.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# í‰ê°€ì§€ ë§Œë“¤ê¸° \n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "# âœ… OpenAI client (í™˜ê²½ë³€ìˆ˜ë¡œ í‚¤ ê´€ë¦¬ ì¤‘)\n",
    "client = OpenAI()\n",
    "\n",
    "# âœ… 1. ì²­í¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"output/combined_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# âœ… 2. ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "def build_prompt(text):\n",
    "    return f\"\"\"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µ ìŒì„ 1~2ê°œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸:\n",
    "{text}\n",
    "\n",
    "í˜•ì‹: \n",
    "[\n",
    "  {{\"question\": \"ì§ˆë¬¸ ë‚´ìš©\", \"answer\": \"ì •ë‹µ ë‚´ìš©\"}}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# âœ… 3. ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ (GPT í˜¸ì¶œ)\n",
    "def generate_qa_from_text(text):\n",
    "    prompt = build_prompt(text)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ í‰ê°€ìš© ì§ˆë¬¸ì§€ë¥¼ ë§Œë“œëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        qa_pairs = json.loads(content)\n",
    "        return qa_pairs\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì˜¤ë¥˜ ë°œìƒ:\", e)\n",
    "        return []\n",
    "\n",
    "# âœ… 4. ì „ì²´ QA ìƒì„± ì‹¤í–‰\n",
    "qa_dataset = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    text = chunk[\"text\"]\n",
    "    qa_pairs = generate_qa_from_text(text)\n",
    "    for qa in qa_pairs:\n",
    "        if qa.get(\"question\") and qa.get(\"answer\"):\n",
    "            qa_dataset.append(qa)\n",
    "    time.sleep(0.5)  # ğŸ’¡ rate limit ë°©ì§€ (í•„ìš”ì‹œ ì¡°ì •)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"ğŸ”„ {i}/{len(chunks)}ê°œ ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "# âœ… 5. ì €ì¥\n",
    "save_path = \"output/eval_questions_gpt.jsonl\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_dataset:\n",
    "        f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… GPT ê¸°ë°˜ ì§ˆë¬¸ì§€ {len(qa_dataset)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm3)",
   "language": "python",
   "name": "llm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
