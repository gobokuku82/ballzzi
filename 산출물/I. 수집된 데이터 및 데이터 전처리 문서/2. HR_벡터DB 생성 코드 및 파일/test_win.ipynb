{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í‰ê°€\n",
    "1. ë‚´ë¶€ê·œì • í‰ê°€\n",
    "    - ì²­í¬ë³´ì™„ ë°©ì‹ : ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹\n",
    "    - ì„ë² ë”© ëª¨ë¸ : KURE-V1\n",
    "    - ë¦¬ë­í¬ ëª¨ë¸ : BGE-RERANKER-KO\n",
    "    - ì§ˆë¬¸ì§€ : GPT-4O ìƒì„± 132ê°œì§ˆë¬¸ì§€\n",
    "    - LLM : GPT-4O-mini\n",
    "    - í”„ë¡¬í”„íŠ¸ 5ì¢…ì— ë”°ë¥¸ ì ìˆ˜í‰ê°€\n",
    "2. í‰ê°€ ê²°ê³¼\n",
    "    - ì²­í¬ë³´ì™„ì„ë² ë”© + gpt - f1/em  : 40.4433/0.000040\n",
    "    - ì²­í¬ë³´ì™„ì„ë² ë”© + gpt + ë¦¬ë­ì»¤ + ê¸ˆì§€í˜• í”„ë¡¬í”„íŠ¸ - f1/em : 55.04/10.61\n",
    "    - ì²­í¬ë³´ì™„ì„ë² ë”© + gpt + ë¦¬ë­ì»¤ + 4ê°€ì§€ í”„ë¡¬í”„íŠ¸ - f1/em \n",
    "      : 43.17/0.76 47.58/6.82 46.91/0.00 14.6/00.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdy\\anaconda3\\envs\\llm3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\kdy\\AppData\\Local\\Temp\\ipykernel_23076\\794522863.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ì„¤ì •\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import evaluate\n",
    "\n",
    "client = OpenAI(api_key=\"Key\")  \n",
    "\n",
    "# ì„ë² ë”©ëª¨ë¸ ë¡œë“œ ( í—ˆê¹…í˜ì´ìŠ¤ )\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"work1/models/kure_v1\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š F1 í‰ê· : 40.4433\n",
      "ğŸ“Š EM í‰ê· : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 1. ì²­í¬ë³´ì™„ì„ë² ë”© + gpt - f1/em  : 40.4433/0.000040\n",
    "\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import evaluate\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"work1/models/kure_v1\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 2. FAISS ë²¡í„° DB ë¡œë“œ\n",
    "vectorstore = FAISS.load_local(\n",
    "    folder_path=\"faiss_win\",\n",
    "    embeddings=embedding,\n",
    "    index_name=\"index\",\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 3. í‰ê°€ ì§ˆë¬¸ì§€ ë¡œë“œ\n",
    "with open(\"eval_questions_window.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_pairs = [json.loads(line) for line in f]\n",
    "\n",
    "# 4. GPT-4o í˜¸ì¶œ í•¨ìˆ˜ (ìˆœì°¨ ì²˜ë¦¬, Top-k=1)\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# GPT-4o í˜¸ì¶œ í•¨ìˆ˜ (ë™ê¸° ë²„ì „)\n",
    "def ask(question, context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì •ì— ëŒ€í•´ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œì…ë‹ˆë‹¤:\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"ì§ˆë¬¸: {question}\"}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ì „ì²´ ì‹¤í–‰ ë£¨í”„ (ë™ê¸° ì²˜ë¦¬)\n",
    "results = []\n",
    "for qa in qa_pairs:\n",
    "    docs = vectorstore.similarity_search(qa[\"question\"], k=1)\n",
    "    context = docs[0].page_content if docs else \"\"\n",
    "    gen = ask(qa[\"question\"], context)\n",
    "    results.append(gen)\n",
    "\n",
    "\n",
    "# 5. SQuAD í‰ê°€\n",
    "squad = evaluate.load(\"squad\")\n",
    "predictions = [{\"id\": str(i), \"prediction_text\": gen} for i, gen in enumerate(results)]\n",
    "references = [{\"id\": str(i), \"answers\": {\"text\": [qa[\"answer\"]], \"answer_start\": [0]}} for i, qa in enumerate(qa_pairs)]\n",
    "\n",
    "score = squad.compute(predictions=predictions, references=references)\n",
    "print(f\"ğŸ“Š F1 í‰ê· : {score['f1']:.4f}\")\n",
    "print(f\"ğŸ“Š EM í‰ê· : {score['exact_match']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [08:21<00:00,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í‰ê·  F1: 55.04\n",
      "ğŸ“Š í‰ê·  EM: 10.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ì²­í¬ë³´ì™„ì„ë² ë”© + gpt + ë¦¬ë­ì»¤ + ê¸ˆì§€í˜• í”„ë¡¬í”„íŠ¸ - f1/em : 55.04/10.61\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from evaluate import load\n",
    "from openai import OpenAI\n",
    "\n",
    "# ğŸ”¹ ì´ë¯¸ ì„ ì–¸ëœ ê°ì²´ë“¤ (í•„ìš”ì‹œ ìˆ˜ì •)\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"work1/models/kure_v1\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 1. ë¦¬ë­ì»¤ ë¡œë“œ (bge-reranker-v2-m3-ko)\n",
    "reranker = CrossEncoder(\"work1/models/bge-reranker-v2-m3-ko\", device=\"cuda\")\n",
    "\n",
    "# 2. FAISS ë²¡í„°DB ë¡œë“œ\n",
    "vectorstore = FAISS.load_local(\n",
    "    folder_path=\"faiss_win\",\n",
    "    embeddings=embedding,\n",
    "    index_name=\"index\",\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 3. í‰ê°€ ì§ˆë¬¸ ë¡œë“œ\n",
    "with open(\"eval_questions_window.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_questions = [json.loads(line) for line in f]\n",
    "\n",
    "# 4. í‰ê°€ ì§€í‘œ ì¤€ë¹„\n",
    "f1 = load(\"evaluate-metric/squad\", \"f1\")\n",
    "em = load(\"evaluate-metric/squad\", \"exact_match\")\n",
    "\n",
    "f1_scores, em_scores = [], []\n",
    "\n",
    "# 5. í‰ê°€ ë£¨í”„\n",
    "for idx, qa in tqdm(enumerate(eval_questions), total=len(eval_questions)):\n",
    "    query = qa[\"question\"]\n",
    "    answer = qa[\"answer\"]\n",
    "    qid = qa.get(\"id\", str(idx))  # ID ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©\n",
    "\n",
    "    # Step 1: Top-5 ë¬¸ì„œ ê²€ìƒ‰\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "    # Step 2: rerankerë¡œ ì ìˆ˜ ê³„ì‚°\n",
    "    reranker_inputs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = reranker.predict(reranker_inputs)\n",
    "\n",
    "    # Step 3: ì ìˆ˜ ê¸°ë°˜ ì •ë ¬\n",
    "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc = reranked[0][0].page_content  # ê°€ì¥ ë†’ì€ ì ìˆ˜ 1ê°œ ì‚¬ìš©\n",
    "\n",
    "    # Step 4: GPT-4o í˜¸ì¶œ (í”„ë¡¬í”„íŠ¸ ê°œì„  í¬í•¨)\n",
    "    system_prompt = (\n",
    "        \"ë„ˆëŠ” íšŒì‚¬ì˜ ì‚¬ë‚´ ê·œì •ì„ ì •í™•íˆ ì•ˆë‚´í•˜ëŠ” QA ë¹„ì„œì•¼. \"\n",
    "        \"ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì— ê¸°ë°˜í•´ì„œë§Œ ë‹µë³€í•´. \"\n",
    "        \"ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ê³  'ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•´.\"\n",
    "    )\n",
    "    user_prompt = f\"\"\"ë¬¸ì„œ:\n",
    "{top_doc}\n",
    "\n",
    "ì§ˆë¬¸: {query}\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    prediction = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Step 5: í‰ê°€ í¬ë§· ë³€í™˜ ë° F1/EM ê³„ì‚°\n",
    "    prediction_dict = {\"id\": qid, \"prediction_text\": prediction}\n",
    "    reference_dict = {\n",
    "        \"id\": qid,\n",
    "        \"answers\": [{\"text\": answer, \"answer_start\": 0}]\n",
    "    }\n",
    "\n",
    "    f1_score = f1.compute(predictions=[prediction_dict], references=[reference_dict])[\"f1\"]\n",
    "    em_score = em.compute(predictions=[prediction_dict], references=[reference_dict])[\"exact_match\"]\n",
    "\n",
    "    f1_scores.append(f1_score)\n",
    "    em_scores.append(em_score)\n",
    "\n",
    "# 6. ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ“Š í‰ê·  F1: {sum(f1_scores)/len(f1_scores):.2f}\")\n",
    "print(f\"ğŸ“Š í‰ê·  EM: {sum(em_scores)/len(em_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ì‹¤í—˜ í”„ë¡¬í”„íŠ¸: basic\n",
      "ğŸ“Š í‰ê·  F1: 43.17\n",
      "ğŸ“Š í‰ê·  EM: 0.76\n",
      "\n",
      "ğŸ” ì‹¤í—˜ í”„ë¡¬í”„íŠ¸: strict\n",
      "ğŸ“Š í‰ê·  F1: 47.58\n",
      "ğŸ“Š í‰ê·  EM: 6.82\n",
      "\n",
      "ğŸ” ì‹¤í—˜ í”„ë¡¬í”„íŠ¸: cot\n",
      "ğŸ“Š í‰ê·  F1: 46.91\n",
      "ğŸ“Š í‰ê·  EM: 0.00\n",
      "\n",
      "ğŸ” ì‹¤í—˜ í”„ë¡¬í”„íŠ¸: step\n",
      "ğŸ“Š í‰ê·  F1: 14.60\n",
      "ğŸ“Š í‰ê·  EM: 0.00\n"
     ]
    }
   ],
   "source": [
    "# ì²­í¬ë³´ì™„ì„ë² ë”© + gpt + ë¦¬ë­ì»¤ + 4ê°€ì§€ í”„ë¡¬í”„íŠ¸ - f1/em \n",
    "\n",
    "from rag_eval import run_prompt_ab_test\n",
    "\n",
    "run_prompt_ab_test(\n",
    "    client=client,\n",
    "    embedding=embedding,\n",
    "    prompt_styles=[\"basic\", \"strict\", \"cot\", \"step\"],  # ì‹¤í—˜í•  í”„ë¡¬í”„íŠ¸ ì¢…ë¥˜\n",
    "    top_k=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
