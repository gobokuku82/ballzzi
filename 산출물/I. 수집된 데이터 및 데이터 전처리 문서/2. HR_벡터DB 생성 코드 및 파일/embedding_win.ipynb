{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB êµ¬ì¶• \n",
    "1. ë°ì´í„° í™•ì¸ - í…ìŠ¤íŠ¸,ìì—°ì–´,í…Œì´ë¸”ë°ì´í„° í˜¼í•©í˜• / ì½”ë“œí™•ì¸ ë° ìˆ˜ë™ìœ¼ë¡œ í™•ì¸ \n",
    "2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ê¸°ë³¸ ì²­í¬ ì§„í–‰ \n",
    "3. ì²­í¬ í™•ì¸ \n",
    "4. ì²­í¬ ë³´ì™„ì‘ì—… ( ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ) \n",
    "5. ì„ë² ë”© (kure-v1 / faiss db) \n",
    "6. ì§ˆë¬¸ì§€ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ ë¬¸ë‹¨ ìˆ˜: 223\n",
      "00: 'DM_solution ì‚¬ë‚´ê·œì •'\n",
      "01: ''\n",
      "02: 'ëª©ì°¨'\n",
      "03: 'ì¶œì¥ ë° ì—¬ë¹„ê·œì •'\n",
      "04: 'í‡´ì§ê¸ˆ ì§€ê¸‰ ë° í‡´ì§ ê·œì •'\n",
      "05: 'ë³µë¦¬í›„ìƒ ê·œì •'\n",
      "06: 'ë³µë¬´ ê·œì •'\n",
      "07: 'ê¸‰ì—¬ ê·œì •'\n",
      "08: 'ì—…ë¬´ ê·œì •'\n",
      "09: 'ì •ë³´ë³´ì•ˆ ê·œì •)'\n",
      "10: 'ìœ¤ë¦¬ ë° í–‰ë™ê°•ë ¹'\n",
      "11: 'ì‚¬ë‚´ ì—¬ë¹„ê·œì • (ì¶œì¥ê·œì •)'\n",
      "12: 'ì œ1ì¥ ì´ ì¹™'\n",
      "13: 'ì œ1ì¡°ã€ëª©ì ã€‘\n",
      "ì´ ê·œì •ì€ ì§ì›ì˜ êµ­ë‚´ ë° í•´ì™¸ ì¶œì¥ì— ë”°ë¥¸ ì—¬ë¹„ ì§€ê¸‰ ë° ì²˜ë¦¬ ê¸°ì¤€ì„ ì •í•¨ìœ¼ë¡œì¨, ê²½ë¹„ì˜ í•©ë¦¬ì  ì§€ì¶œê³¼ ì¶œì¥ì—…ë¬´ì˜ ì›í™œí•œ ìˆ˜í–‰ì„ ë„ëª¨í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.'\n",
      "14: 'ì œ2ì¡°ã€ì ìš©ë²”ìœ„ã€‘\n",
      "ì´ ê·œì •ì€ ë³¸ì‚¬ ë° êµ­ë‚´Â·í•´ì™¸ ì§€ì‚¬ ì†Œì† ì „ ì§ì›ì—ê²Œ ì ìš©í•œë‹¤. ë‹¨, ê³„ì•½ì§ ë° ì¸í„´ì‚¬ì›ì€ ë³„ë„ì˜ ìŠ¹ì¸ ì—†ì´ ì¶œì¥í•  ìˆ˜ ì—†ë‹¤.'\n",
      "15: 'ì œ3ì¡°ã€ì •ì˜ã€‘\n",
      "ì´ ê·œì •ì—ì„œ â€œì¶œì¥â€ì´ë¼ í•¨ì€ íšŒì‚¬ì˜ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•˜ì—¬ ë³¸ë˜ ê·¼ë¬´ì§€ë¥¼ ë– ë‚˜ ì¼ì‹œì ìœ¼ë¡œ ë‹¤ë¥¸ ì§€ì—­ì— ì²´ë¥˜í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.'\n",
      "16: ''\n",
      "17: 'ì œ2ì¥ ì¶œì¥ ìŠ¹ì¸ ë° ì‹ ì²­'\n",
      "18: 'ì œ4ì¡°ã€ì¶œì¥ ìŠ¹ì¸ã€‘\n",
      "ëª¨ë“  ì¶œì¥ì€ ì‚¬ì „ì— ì†Œì† ë¶€ì„œì¥ê³¼ ê²½ì˜ì§€ì›íŒ€ì˜ ìŠ¹ì¸ì„ ë°›ì•„ì•¼ í•˜ë©°, ìŠ¹ì¸ëœ ì¶œì¥ë§Œ ì—¬ë¹„ê°€ ì§€ê¸‰ëœë‹¤.'\n",
      "19: 'ì œ5ì¡°ã€ì¶œì¥ ì‹ ì²­ ì ˆì°¨ã€‘\n",
      "â‘  ì¶œì¥ì‹ ì²­ì„œëŠ” ì¶œì¥ 3ì¼ ì „ê¹Œì§€ ì‹œìŠ¤í…œì— ë“±ë¡í•´ì•¼ í•œë‹¤.\n",
      "â‘¡ í•´ì™¸ì¶œì¥ì˜ ê²½ìš°, ë³„ë„ì˜ ì‚¬ì „ë³´ê³ ì„œì™€ í˜„ì§€ ì—°ë½ì²˜ê°€ í¬í•¨ëœ ê³„íšì„œë¥¼ ì²¨ë¶€í•´ì•¼ í•œë‹¤.'\n",
      "20: ''\n",
      "21: 'ì œ3ì¥ ì—¬ë¹„ì˜ ì¢…ë¥˜ ë° ê¸°ì¤€'\n",
      "22: 'ì œ6ì¡°ã€ì—¬ë¹„ êµ¬ì„± í•­ëª©ã€‘\n",
      "ì—¬ë¹„ëŠ” ë‹¤ìŒ ê° í•­ëª©ìœ¼ë¡œ êµ¬ì„±í•œë‹¤.'\n",
      "23: 'êµí†µë¹„'\n",
      "24: 'ì¼ë¹„(ìˆ™ë°• ì™¸ ì²´ì¬ë¹„)'\n",
      "25: 'ìˆ™ë°•ë¹„'\n",
      "26: 'ì‹ë¹„'\n",
      "27: 'í†µì‹ ë¹„ ë° ì¡ë¹„(í•„ìš” ì‹œ ì¸ì •)'\n",
      "28: 'ì œ7ì¡°ã€êµí†µë¹„ ì§€ê¸‰ ê¸°ì¤€ã€‘\n",
      "â‘  êµ­ë‚´ ì¶œì¥ ì‹œì—ëŠ” ëŒ€ì¤‘êµí†µì„ ì›ì¹™ìœ¼ë¡œ í•œë‹¤.\n",
      "â‘¡ ìê°€ìš© ì‚¬ìš© ì‹œ ì‚¬ì „ ìŠ¹ì¸ê³¼ ì£¼í–‰ê±°ë¦¬ ì¦ë¹™ìë£Œ(ë„¤ë¹„ê²Œì´ì…˜ ìº¡ì²˜ ë“±)ë¥¼ ì²¨ë¶€í•´ì•¼ í•œë‹¤.\n",
      "â‘¢ í•´ì™¸ì¶œì¥ ì‹œì—ëŠ” ì´ì½”ë…¸ë¯¸ í´ë˜ìŠ¤ë§Œ ì´ìš©ê°€ëŠ¥í•˜ë‹¤. ì‚¬ì—… ëª©ì ì˜ ë™ë°˜ìê°€ ìˆì„ ê²½ìš°ì—ëŠ” í¼ìŠ¤íŠ¸ í´ë˜ìŠ¤ ì´ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. ( CEOí¬í•¨ ì „ ì§ì› ê³µí†µ )'\n",
      "29: 'ì œ8ì¡°ã€ìˆ™ë°•ë¹„ ì§€ê¸‰ ê¸°ì¤€ã€‘\n",
      "â‘  êµ­ë‚´ ì¶œì¥: 1ë°•ë‹¹ 100,000ì› ì´ë‚´ ì‹¤ë¹„ ì •ì‚°\n",
      "â‘¡ í•´ì™¸ ì¶œì¥: êµ­ê°€ë³„ ìƒí•œì„ ì„ ë‘ë©°, ì™¸êµë¶€ ê³µë¬´ ì¶œì¥ ê¸°ì¤€ì„ ì°¸ê³ í•œë‹¤.\n",
      "â‘¢ ë‹¨, í˜¸í…” ì´ìš© ì‹œ ë°˜ë“œì‹œ ì¹´ë“œ ì˜ìˆ˜ì¦ê³¼ í˜¸í…” ëª…ì„¸ì„œë¥¼ ì œì¶œí•´ì•¼ í•œë‹¤.'\n"
     ]
    }
   ],
   "source": [
    "# 1. ë°ì´í„° í™•ì¸\n",
    "\n",
    "from docx import Document\n",
    "from pathlib import Path\n",
    "\n",
    "# ğŸ“‚ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = Path(\"work1/data/DM_rules.docx\").resolve()\n",
    "\n",
    "# ğŸ“„ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "doc = Document(file_path)\n",
    "\n",
    "# ğŸ“‘ ì „ì²´ ë¬¸ë‹¨ í™•ì¸\n",
    "print(f\"âœ… ì´ ë¬¸ë‹¨ ìˆ˜: {len(doc.paragraphs)}\")\n",
    "\n",
    "# ğŸ” ì•ë¶€ë¶„ 30ê°œ ë¬¸ë‹¨ ë¯¸ë¦¬ë³´ê¸°\n",
    "for i, para in enumerate(doc.paragraphs[:30]):\n",
    "    print(f\"{i:02d}: '{para.text.strip()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ ìš”ì†Œ ìˆ˜ (ë¬¸ë‹¨+í‘œ í¬í•¨): 182\n",
      "âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: 42\n",
      "ğŸ“ ì €ì¥ ì™„ë£Œ: dm_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ê¸°ë³¸ ì²­í¬ ì§„í–‰\n",
    "\n",
    "from docx import Document\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "docx_path = Path(\"work1/data/DM_rules.docx\")  # ì‹¤ì œ ì „ì²´ ë¬¸ì„œ\n",
    "output_path = Path(\"work1/data/dm_chunks.json\")\n",
    "\n",
    "# 2. ë¬¸ì„œ ë¡œë“œ\n",
    "doc = Document(docx_path)\n",
    "\n",
    "# 3. ë¬¸ë‹¨ + í‘œ í¬í•¨ ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
    "elements = []\n",
    "\n",
    "# ë¬¸ë‹¨ ì¶”ê°€\n",
    "for para in doc.paragraphs:\n",
    "    text = para.text.strip()\n",
    "    if text:\n",
    "        elements.append(text)\n",
    "\n",
    "# í‘œ ì¶”ì¶œ\n",
    "for table in doc.tables:\n",
    "    table_text = []\n",
    "    for row in table.rows:\n",
    "        cells = [cell.text.strip() for cell in row.cells]\n",
    "        table_text.append(\" | \".join(cells))\n",
    "    elements.append(\"\\n\".join(table_text))\n",
    "\n",
    "# 4. ì¡°ë¬¸ ê¸°ì¤€ ì „ì²˜ë¦¬\n",
    "chunks = []\n",
    "current_title = \"\"\n",
    "current_body = []\n",
    "\n",
    "for line in elements:\n",
    "    if re.match(r\"^ì œ\\d+ì¡°[ã€\\[ ].*?[ã€‘\\]]\", line):  # ìƒˆë¡œìš´ ì¡°ë¬¸ ì‹œì‘\n",
    "        if current_title and current_body:\n",
    "            chunks.append({\n",
    "                \"text\": \"\\n\".join(current_body),\n",
    "                \"metadata\": {\n",
    "                    \"section\": current_title,\n",
    "                    \"chunk_id\": f\"{current_title}_{str(uuid4())[:8]}\"\n",
    "                }\n",
    "            })\n",
    "        current_title = line\n",
    "        current_body = []\n",
    "    else:\n",
    "        current_body.append(line)\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€\n",
    "if current_title and current_body:\n",
    "    chunks.append({\n",
    "        \"text\": \"\\n\".join(current_body),\n",
    "        \"metadata\": {\n",
    "            \"section\": current_title,\n",
    "            \"chunk_id\": f\"{current_title}_{str(uuid4())[:8]}\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "# 5. ì €ì¥\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 6. í†µê³„ ì¶œë ¥\n",
    "print(f\"âœ… ì´ ìš”ì†Œ ìˆ˜ (ë¬¸ë‹¨+í‘œ í¬í•¨): {len(elements)}\")\n",
    "print(f\"âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
    "print(f\"ğŸ“ ì €ì¥ ì™„ë£Œ: {output_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ ì²­í¬ ìˆ˜: 42\n",
      "âŒ ì¤‘ë³µ chunk_id ìˆ˜: 0\n",
      "ğŸ“ í‰ê·  ê¸¸ì´: 38ì\n",
      "ğŸ“ ìµœëŒ€ ê¸¸ì´: 515 / ìµœì†Œ ê¸¸ì´: 8\n",
      "\n",
      "ğŸ“š ì¡°ë¬¸ í˜•ì‹ ë§Œì¡± ì²­í¬ ìˆ˜: 42\n",
      "âœ… ëª¨ë“  ì²­í¬ê°€ ì¡°ë¬¸ í˜•ì‹ì„ ë§Œì¡±í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 3. ì²­í¬ í™•ì¸ \n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# íŒŒì¼ ë¡œë“œ\n",
    "with open(\"work1/data/dm_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# ì¤‘ë³µ ID ì²´í¬\n",
    "ids = [chunk[\"metadata\"][\"chunk_id\"] for chunk in chunks]\n",
    "dup_ids = [item for item, count in Counter(ids).items() if count > 1]\n",
    "\n",
    "# ê¸¸ì´ í†µê³„\n",
    "lengths = [len(chunk[\"text\"]) for chunk in chunks]\n",
    "avg_len = sum(lengths) // len(lengths)\n",
    "min_len = min(lengths)\n",
    "max_len = max(lengths)\n",
    "\n",
    "# ì¡°ë¬¸ í˜•ì‹ ì²´í¬\n",
    "pattern = r\"ì œ\\d+ì¡°[ã€\\[ ].*?[ã€‘\\]]\"\n",
    "valid_titles = [chunk[\"metadata\"][\"section\"] for chunk in chunks if re.search(pattern, chunk[\"metadata\"][\"section\"])]\n",
    "invalid_titles = [chunk[\"metadata\"][\"section\"] for chunk in chunks if not re.search(pattern, chunk[\"metadata\"][\"section\"])]\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"âœ… ì´ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
    "print(f\"âŒ ì¤‘ë³µ chunk_id ìˆ˜: {len(dup_ids)}\")\n",
    "print(f\"ğŸ“ í‰ê·  ê¸¸ì´: {avg_len}ì\")\n",
    "print(f\"ğŸ“ ìµœëŒ€ ê¸¸ì´: {max_len} / ìµœì†Œ ê¸¸ì´: {min_len}\\n\")\n",
    "\n",
    "print(f\"ğŸ“š ì¡°ë¬¸ í˜•ì‹ ë§Œì¡± ì²­í¬ ìˆ˜: {len(valid_titles)}\")\n",
    "if invalid_titles:\n",
    "    print(f\"âš ï¸ í˜•ì‹ ë¶ˆë§Œì¡± ì²­í¬ ì˜ˆì‹œ: {invalid_titles[:3]}\")\n",
    "else:\n",
    "    print(\"âœ… ëª¨ë“  ì²­í¬ê°€ ì¡°ë¬¸ í˜•ì‹ì„ ë§Œì¡±í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìƒì„± ì™„ë£Œ: ì´ 88ê°œ\n",
      "ğŸ“ í‰ê·  ê¸¸ì´: 303ì\n",
      "ğŸ“ ìµœëŒ€ ê¸¸ì´: 527 / ìµœì†Œ ê¸¸ì´: 180\n"
     ]
    }
   ],
   "source": [
    "# 4. ì²­í¬ ë³´ì™„ì‘ì—… ( ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ) \n",
    "\n",
    "import json\n",
    "\n",
    "# 1. ê¸°ì¡´ ì²­í¬ ë¡œë“œ\n",
    "with open(\"dm_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# 2. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë³‘í•©\n",
    "WINDOW_SIZE = 3\n",
    "STRIDE = 1\n",
    "windowed_chunks = []\n",
    "\n",
    "for i in range(0, len(chunks) - WINDOW_SIZE + 1, STRIDE):\n",
    "    group = chunks[i:i + WINDOW_SIZE]\n",
    "    combined_text = \"\\n\".join([c[\"text\"] for c in group])\n",
    "    section = group[0][\"metadata\"][\"section\"]  \n",
    "    chunk_id = f\"{section}_win_{i+1}\"\n",
    "\n",
    "    windowed_chunks.append({\n",
    "        \"text\": combined_text,\n",
    "        \"metadata\": {\n",
    "            \"section\": section,\n",
    "            \"chunk_id\": chunk_id\n",
    "        }\n",
    "    })\n",
    "\n",
    "# 3. ì €ì¥\n",
    "with open(\"dm_chunks_window.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(windowed_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 4. í™•ì¸\n",
    "lengths = [len(c[\"text\"]) for c in windowed_chunks]\n",
    "print(f\"âœ… ìƒì„± ì™„ë£Œ: ì´ {len(windowed_chunks)}ê°œ\")\n",
    "print(f\"ğŸ“ í‰ê·  ê¸¸ì´: {sum(lengths)//len(lengths)}ì\")\n",
    "print(f\"ğŸ“ ìµœëŒ€ ê¸¸ì´: {max(lengths)} / ìµœì†Œ ê¸¸ì´: {min(lengths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ ì„ë² ë”© ì¤‘...\n",
      "âœ… ì €ì¥ ì™„ë£Œ: faiss_win\n"
     ]
    }
   ],
   "source": [
    "# 5. ì„ë² ë”© ( ì²­í¬ë³´ì™„íŒŒì¼ ,kure-v1 , faiss db) \n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ğŸŸ¦ 1. ì„¤ì •\n",
    "MODEL_NAME = \"work1\\models\\kure_v1\"\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 32\n",
    "DATA_FILE = \"dm_chunks_window.json\"  \n",
    "INDEX_DIR = \"faiss_win\"  \n",
    "\n",
    "# ğŸŸ¦ 2. JSON ë¡œë“œ\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=item[\"text\"],\n",
    "        metadata=item[\"metadata\"]\n",
    "    ) for item in data\n",
    "]\n",
    "\n",
    "# ğŸŸ¦ 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_kwargs={\"device\": DEVICE},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": BATCH_SIZE}\n",
    ")\n",
    "\n",
    "# ğŸŸ¦ 4. ì„ë² ë”© + ì €ì¥\n",
    "print(\"â³ ì„ë² ë”© ì¤‘...\")\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "vectorstore.save_local(INDEX_DIR)\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {INDEX_DIR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [01:54<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ 132ê°œ ë¬¸í•­ â†’ eval_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. í‰ê°€ë¥¼ ìœ„í•œ ì§ˆë¬¸ì§€ ìƒì„± ( openai gpt-4o )\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key=\"Key\")\n",
    "                \n",
    "# 1. ì²­í¬ ë¡œë“œ\n",
    "with open(\"dm_chunks_merged.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "questions = []\n",
    "\n",
    "# 2. ê³ í’ˆì§ˆ ì§ˆë¬¸ ìœ ë„ í”„ë¡¬í”„íŠ¸ ìƒì„± + ìš”ì²­\n",
    "for chunk in tqdm(chunks):\n",
    "    prompt = f\"\"\"\n",
    "ë‹¤ìŒì€ ì‚¬ë‚´ ê·œì •ì˜ í•œ ì¡°ë¬¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì— ê¸°ë°˜í•˜ì—¬ ì‹¤ì œ ì§ì›ë“¤ì´ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ 3ê°œ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "[ê·œì • ë‚´ìš©]\n",
    "\\\"\\\"\\\"\n",
    "{chunk[\"text\"]}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "ì¡°ê±´:\n",
    "- ì‹¤ì œ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¼ ë²•í•œ ì§ˆë¬¸ì¼ ê²ƒ\n",
    "- ìˆ«ì, ì¡°ê±´ë¬¸, ì˜ˆì™¸ì‚¬í•­ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ë°˜ì˜í•  ê²ƒ\n",
    "- ì§ˆë¬¸ì€ í•œêµ­ì–´ë¡œ, ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ì‘ì„±\n",
    "- ë‹µë³€ì€ ë°˜ë“œì‹œ ìœ„ í…ìŠ¤íŠ¸ ë‚´ì—ì„œë§Œ ì¶”ì¶œ ê°€ëŠ¥í•´ì•¼ í•¨\n",
    "\n",
    "í˜•ì‹:\n",
    "[\n",
    "  {{\"question\": \"...\", \"answer\": \"...\"}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        raw = response.choices[0].message.content\n",
    "        qa_list = json.loads(raw)\n",
    "        for qa in qa_list:\n",
    "            if qa[\"question\"].strip() and qa[\"answer\"].strip():\n",
    "                questions.append(qa)\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸ Error] ì²­í¬ ID: {chunk['metadata']['chunk_id']} â†’ {e}\")\n",
    "\n",
    "# 3. ì €ì¥\n",
    "with open(\"eval_questions_merged.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for q in questions:\n",
    "        f.write(json.dumps(q, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: ì´ {len(questions)}ê°œ ë¬¸í•­ â†’ eval_questions_merged.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ GPT ì‘ë‹µ ë‚´ìš© í™•ì¸:\n",
      "'[\\n  {\"question\": \"ì„œì•½ì„œë¥¼ ì œì¶œí•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\", \"answer\": \"ëª¨ë“  ì„ì§ì›ì€ ë³¸ ê°•ë ¹ì„ ìˆ™ì§€í•˜ê³  ì„œì•½ì„œë¥¼ ì œì¶œí•´ì•¼ í•˜ë©°, ìœ„ë°˜ ì‹œ ì±…ì„ì„ ì§„ë‹¤.\"},\\n  {\"question\": \"ì„œì•½ì„œëŠ” ì–¼ë§ˆë‚˜ ì˜¤ë«ë™ì•ˆ ë³´ê´€ë˜ë‚˜ìš”?\", \"answer\": \"ì„œì•½ì„œëŠ” ì¸ì‚¬ê¸°ë¡ê³¼ í•¨ê»˜ 3ë…„ê°„ ë³´ê´€ëœë‹¤.\"},\\n  {\"question\": \"ì„œì•½ì„œë¥¼ ì œì¶œí•´ì•¼ í•˜ëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€ìš”?\", \"answer\": \"ëª¨ë“  ì„ì§ì›ì€ ë³¸ ê°•ë ¹ì„ ìˆ™ì§€í•˜ê³  ì„œì•½ì„œë¥¼ ì œì¶œí•´ì•¼ í•œë‹¤.\"}\\n]'\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸ì§€ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "print(\"ğŸ“¤ GPT ì‘ë‹µ ë‚´ìš© í™•ì¸:\")\n",
    "print(repr(response.choices[0].message.content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
