{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í‰ê°€\n",
    "1. ì¸ì‚¬ì •ë³´ í‰ê°€\n",
    "    - ì„ë² ë”© ëª¨ë¸ : KURE-V1\n",
    "    - ë¦¬ë­í¬ ëª¨ë¸ : BGE-RERANKER-KO\n",
    "    - ì§ˆë¬¸ì§€ : GPT-4O ìƒì„± 102ê°œì§ˆë¬¸ì§€\n",
    "    - LLM : GPT-4O-mini\n",
    "    - topk1~5 ì¸¡ì •\n",
    "\n",
    "2. í‰ê°€ ê²°ê³¼\n",
    "    - topk 1~5 F1/EM score : 41.11/34.04 46.52/37.77 48.09/38.30 46.62/38.30 48.59/39.36\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdy\\AppData\\Local\\Temp\\ipykernel_29600\\794522863.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ì„¤ì •\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import evaluate\n",
    "\n",
    "client = OpenAI(api_key=\"key\")  \n",
    "\n",
    "# ì„ë² ë”©ëª¨ë¸ ë¡œë“œ ( í—ˆê¹…í˜ì´ìŠ¤ )\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"work1/models/kure_v1\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [ì‹¤í—˜] Top-1 ë¬¸ì„œ â†’ ë¦¬ë­ì»¤ â†’ GPT ì‘ë‹µ í‰ê°€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [03:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í‰ê·  F1: 41.11\n",
      "ğŸ“Š í‰ê·  EM: 34.04\n",
      "\n",
      "ğŸ” [ì‹¤í—˜] Top-2 ë¬¸ì„œ â†’ ë¦¬ë­ì»¤ â†’ GPT ì‘ë‹µ í‰ê°€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [02:34<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í‰ê·  F1: 46.52\n",
      "ğŸ“Š í‰ê·  EM: 37.77\n",
      "\n",
      "ğŸ” [ì‹¤í—˜] Top-3 ë¬¸ì„œ â†’ ë¦¬ë­ì»¤ â†’ GPT ì‘ë‹µ í‰ê°€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [03:15<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í‰ê·  F1: 48.09\n",
      "ğŸ“Š í‰ê·  EM: 38.30\n",
      "\n",
      "ğŸ” [ì‹¤í—˜] Top-4 ë¬¸ì„œ â†’ ë¦¬ë­ì»¤ â†’ GPT ì‘ë‹µ í‰ê°€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [06:20<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í‰ê·  F1: 46.62\n",
      "ğŸ“Š í‰ê·  EM: 38.30\n",
      "\n",
      "ğŸ” [ì‹¤í—˜] Top-5 ë¬¸ì„œ â†’ ë¦¬ë­ì»¤ â†’ GPT ì‘ë‹µ í‰ê°€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [08:08<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í‰ê·  F1: 48.59\n",
      "ğŸ“Š í‰ê·  EM: 39.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ë‚´ë¶€ ê·œì • í‰ê°€ - ë¦¬ë­ì»¤ top-k ì‹¤í—˜\n",
    "\n",
    "from rag_eval import run_custom_prompt_eval_group\n",
    "\n",
    "my_custom_prompt = (\n",
    "    \"\"\"ì§ˆë¬¸ì— ì •í™•íˆ ëŒ€ì‘í•˜ëŠ” ì •ë‹µë§Œ ë§í•˜ì„¸ìš”. ì˜ˆì‹œ:\n",
    "Q: í‡´ì§ê¸ˆì€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ë‚˜ìš”? â†’ í‰ê· ì„ê¸ˆ\n",
    "Q: ì§€ê¸‰ ëŒ€ìƒì€ ëˆ„êµ¬ì¸ê°€ìš”? â†’ í‡´ì§ì\n",
    "ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹¨ë‹µí˜• ì •ë‹µë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì •ë‹µì´ ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  í•˜ì„¸ìš”.\"\"\"\n",
    ")\n",
    "\n",
    "# âœ… Top-k ì‹¤í—˜ ë£¨í”„\n",
    "for k in range(1, 6):  # top_k=1~5\n",
    "    print(f\"\\nğŸ” [ì‹¤í—˜] Top-{k} ë¬¸ì„œ â†’ ë¦¬ë­ì»¤ â†’ GPT ì‘ë‹µ í‰ê°€\")\n",
    "    run_custom_prompt_eval_group(\n",
    "        client=client,\n",
    "        embedding=embedding,\n",
    "        eval_path=\"eval_questions_gpt.jsonl\",\n",
    "        top_k=k,\n",
    "        system_prompt=my_custom_prompt\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
