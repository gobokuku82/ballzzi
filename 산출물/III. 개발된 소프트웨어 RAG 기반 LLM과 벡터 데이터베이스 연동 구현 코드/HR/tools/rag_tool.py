import os
from typing import List, Dict
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_core.tools import tool
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from dotenv import load_dotenv
import torch
import numpy as np
import requests

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class Reranker:
    def __init__(self, model_name: str = "BAAI/bge-reranker-v2-m3"):
        """
        Reranker ì´ˆê¸°í™”
        ê¸°ë³¸ê°’: BAAI/bge-reranker-v2-m3 (multilingual, í•œêµ­ì–´ ì§€ì›)
        """
        # í—ˆê¹…í˜ì´ìŠ¤ í† í° ê°€ì ¸ì˜¤ê¸°
        hf_token = os.getenv("HUGGINGFACE_TOKEN")
        
        print(f"Reranker ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        if hf_token:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name, token=hf_token)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        self.model.eval()
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print("Reranker ëª¨ë¸ì„ CUDAë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")

    def rerank(self, query: str, documents: List[Document]) -> List[Document]:
        if not documents:
            return documents
            
        pairs = [(query, doc.page_content) for doc in documents]
        features = self.tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)
        
        if torch.cuda.is_available():
            features = {k: v.cuda() for k, v in features.items()}
        
        with torch.no_grad():
            scores = self.model(**features).logits.squeeze()
        
        # ë‹¨ì¼ ë¬¸ì„œì¸ ê²½ìš° ì²˜ë¦¬
        if len(documents) == 1:
            return documents
            
        # ì ìˆ˜ì— ë”°ë¼ ë¬¸ì„œ ì¬ì •ë ¬
        sorted_indices = torch.argsort(scores, descending=True)
        return [documents[i] for i in sorted_indices]

class RAGTool:
    def __init__(self):
        # í—ˆê¹…í˜ì´ìŠ¤ í† í° ê°€ì ¸ì˜¤ê¸°
        hf_token = os.getenv("HUGGINGFACE_TOKEN")
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì„¤ì •)
        embedding_model = os.getenv("EMBEDDING_MODEL", "nlpai-lab/KURE-v1")
        reranker_model = os.getenv("RERANKER_MODEL", "BAAI/bge-reranker-v2-m3")
        
        print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {embedding_model}")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” - í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
        model_kwargs = {"device": "cuda" if torch.cuda.is_available() else "cpu"}
        encode_kwargs = {"normalize_embeddings": True}
        
        # í† í°ì´ ìˆìœ¼ë©´ model_kwargsì— í¬í•¨
        if hf_token:
            model_kwargs["token"] = hf_token
            # í™˜ê²½ë³€ìˆ˜ë¡œë„ ì„¤ì • (ì¼ë¶€ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©)
            os.environ["HF_TOKEN"] = hf_token
        
        # HuggingFaceEmbeddings ì´ˆê¸°í™”
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        
        print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ì—¬ëŸ¬ FAISS ì¸ë±ìŠ¤ ë¡œë“œ - í™˜ê²½ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        self.vectorstores = {}
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ DB ê²½ë¡œë“¤ ê°€ì ¸ì˜¤ê¸°
        main_db_path = os.getenv("VECTOR_DB_PATH", "HR/data/faiss_win")
        hr_db_path = os.getenv("VECTOR_DB_PATH_HR", "HR/data/faiss_org_hr")
        
        db_configs = [
            ("faiss_win", main_db_path),
            ("faiss_org_hr", hr_db_path)
        ]
        
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
        current_dir = os.getcwd()
        print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
        
        for db_name, db_path in db_configs:
            try:
                # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì¶œë ¥
                print(f"[DEBUG] í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")

                # ê²½ë¡œ ì¡°ì • (app í´ë” ì‹¤í–‰ ì‹œ)
                if os.getcwd().endswith("app") or "app" in os.path.basename(os.getcwd()):
                    db_path = f"../{db_path}"
                    print(f"[DEBUG] app í´ë”ì—ì„œ ì‹¤í–‰ ê°ì§€, ê²½ë¡œ ì¡°ì •ë¨: {db_path}")

                absolute_path = os.path.abspath(db_path)
                print(f"[DEBUG] {db_name} ì ˆëŒ€ ê²½ë¡œ: {absolute_path}")
                print(f"[DEBUG] index.faiss ì¡´ì¬? {os.path.exists(os.path.join(absolute_path, 'index.faiss'))}")
                print(f"[DEBUG] index.pkl ì¡´ì¬? {os.path.exists(os.path.join(absolute_path, 'index.pkl'))}")

                vectorstore = FAISS.load_local(
                    absolute_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                self.vectorstores[db_name] = vectorstore
                print(f"[DEBUG] {db_name} ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì„±ê³µ âœ…")
            
            except Exception as e:
                print(f"[âŒ ERROR] {db_name} FAISS ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue

        if not self.vectorstores:
            raise Exception("ëª¨ë“  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        print(f"ë¡œë“œëœ ë²¡í„° DB: {list(self.vectorstores.keys())}")

        # Reranker ì´ˆê¸°í™”
        self.reranker = Reranker(reranker_model)
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def query_rag(self, query: str) -> str:
        try:
            # ëª¨ë“  ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰
            all_docs = []
            top_k = int(os.getenv("TOP_K", "3"))
            
            for db_name, vectorstore in self.vectorstores.items():
                docs = vectorstore.similarity_search(query, k=top_k)
                # DB ì´ë¦„ì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                for doc in docs:
                    if not hasattr(doc, 'metadata'):
                        doc.metadata = {}
                    doc.metadata['source_db'] = db_name
                all_docs.extend(docs)
            
            if not all_docs:
                return f"ì§ˆë¬¸: {query}\n\nê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # Rerankerë¡œ ì¬ì •ë ¬ (ëª¨ë“  DB ê²°ê³¼ í†µí•©)
            reranked_docs = self.reranker.rerank(query, all_docs)
            
            # Top-1 ë¬¸ì„œ ì¶”ì¶œ
            top_doc = reranked_docs[0]
            
            # ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í¬í•¨
            metadata_info = ""
            if hasattr(top_doc, 'metadata') and top_doc.metadata:
                metadata_info = f"\nì¶œì²˜: {top_doc.metadata}\n"
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = f"ì§ˆë¬¸: {query}\n{metadata_info}\nê´€ë ¨ ë¬¸ì„œ:\n{top_doc.page_content}"
            
            return context
            
        except Exception as e:
            return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
print("RAG ë„êµ¬ ì´ˆê¸°í™” ì¤‘...")
rag_tool_instance = RAGTool()

# LangChain ë„êµ¬ë¡œ ë˜í•‘
@tool
def search_documents(query: str) -> str:
    """ë¬¸ì„œ ê²€ìƒ‰ ë° ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ rerankingì„ í†µí•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return rag_tool_instance.query_rag(query) 

@tool
def calculate_retirement_pay(avg_salary: float, years: int) -> str:
    """í‰ê· ì„ê¸ˆê³¼ ê·¼ì†ì—°ìˆ˜ë¡œ í‡´ì§ê¸ˆì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        pay = avg_salary * years
        return f"í‡´ì§ê¸ˆì€ ì•½ {pay:,.0f}ì›ì…ë‹ˆë‹¤."
    except Exception as e:
        return f"ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ë„¤ì´ë²„ ê²€ìƒ‰ API ë„êµ¬
@tool
def search_naver_news(query: str, max_results: int = 3) -> str:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        client_id = os.getenv("NAVER_CLIENT_ID")
        client_secret = os.getenv("NAVER_CLIENT_SECRET")
        
        if not client_id or not client_secret:
            return "ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        params = {
            "query": query,
            "display": max_results,
            "start": 1,
            "sort": "date"  # ìµœì‹ ìˆœ ì •ë ¬
        }
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code == 200:
            data = response.json()
            items = data.get("items", [])
            
            if not items:
                return f"'{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            results = []
            for i, item in enumerate(items, 1):
                title = item.get("title", "").replace("<b>", "").replace("</b>", "")
                description = item.get("description", "").replace("<b>", "").replace("</b>", "")
                link = item.get("link", "")
                pub_date = item.get("pubDate", "")
                
                result = f"{i}. {title}\n{description}\në°œí–‰ì¼: {pub_date}\në§í¬: {link}\n"
                results.append(result)
            
            return f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ('{query}'):\n\n" + "\n".join(results)
        else:
            return f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: HTTP {response.status_code}"
            
    except Exception as e:
        return f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

@tool
def search_naver_web(query: str, max_results: int = 3) -> str:
    """ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì¼ë°˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        client_id = os.getenv("NAVER_CLIENT_ID")
        client_secret = os.getenv("NAVER_CLIENT_SECRET")
        
        if not client_id or not client_secret:
            return "ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        url = "https://openapi.naver.com/v1/search/webkr.json"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        params = {
            "query": query,
            "display": max_results,
            "start": 1
        }
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code == 200:
            data = response.json()
            items = data.get("items", [])
            
            if not items:
                return f"'{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            results = []
            for i, item in enumerate(items, 1):
                title = item.get("title", "").replace("<b>", "").replace("</b>", "")
                description = item.get("description", "").replace("<b>", "").replace("</b>", "")
                link = item.get("link", "")
                
                result = f"{i}. {title}\n{description}\në§í¬: {link}\n"
                results.append(result)
            
            return f"ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ('{query}'):\n\n" + "\n".join(results)
        else:
            return f"ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: HTTP {response.status_code}"
            
    except Exception as e:
        return f"ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ë‚´ë¶€ ë¬¸ì„œ ìš°ì„  í‚¤ì›Œë“œ ëª©ë¡
INTERNAL_PRIORITY_KEYWORDS = [
    # íšŒì‚¬ ê´€ë ¨
    "íšŒì‚¬", "ëŒ€í‘œ", "ì‚¬ì¥", "ì„ì›", "ì§ì›", "ì¡°ì§", "ë¶€ì„œ", "íŒ€",
    # ì¸ì‚¬ ê´€ë ¨  
    "íœ´ê°€", "ì—°ì°¨", "ë³‘ê°€", "ì¶œê·¼", "í‡´ê·¼", "ê·¼ë¬´", "ê¸‰ì—¬", "ì„ê¸ˆ", "í‡´ì§ê¸ˆ", "ìŠ¹ì§„",
    # ê·œì • ê´€ë ¨
    "ê·œì •", "ì •ì±…", "ê·œì¹™", "ì ˆì°¨", "í”„ë¡œì„¸ìŠ¤", "ì—…ë¬´", "ë³´ê³ ",
    # ë³µë¦¬í›„ìƒ
    "ë³µë¦¬í›„ìƒ", "ë³´í—˜", "ì§€ì›ê¸ˆ", "êµìœ¡", "ì—°ìˆ˜"
]

def has_internal_keywords(query: str) -> bool:
    """ì§ˆë¬¸ì— ë‚´ë¶€ ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸"""
    query_lower = query.lower()
    return any(keyword in query_lower for keyword in INTERNAL_PRIORITY_KEYWORDS)

@tool
def hybrid_search(query: str) -> str:
    """ë‚´ë¶€ ë¬¸ì„œë¥¼ ìš°ì„  ê²€ìƒ‰í•˜ê³ , í•„ìš”ì‹œ ë„¤ì´ë²„ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„í•˜ëŠ” í†µí•© ê²€ìƒ‰ ë„êµ¬ì…ë‹ˆë‹¤."""
    try:
        # 1ë‹¨ê³„: ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ (í•­ìƒ ì‹¤í–‰)
        internal_result = rag_tool_instance.query_rag(query)
        
        # 2ë‹¨ê³„: ê²°ê³¼ í’ˆì§ˆ + í‚¤ì›Œë“œ ë¶„ì„
        has_internal_content = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" not in internal_result
        has_internal_keywords_flag = has_internal_keywords(query)
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ ì „ëµ ê²°ì •
        if has_internal_content and has_internal_keywords_flag:
            # ë‚´ë¶€ ë¬¸ì„œì— ê²°ê³¼ê°€ ìˆê³  ë‚´ë¶€ ê´€ë ¨ í‚¤ì›Œë“œë©´ â†’ ë‚´ë¶€ ê²°ê³¼ë§Œ ë°˜í™˜
            return f"ğŸ¢ **íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:**\n\n{internal_result}"
            
        elif has_internal_content and not has_internal_keywords_flag:
            # ë‚´ë¶€ ë¬¸ì„œì— ê²°ê³¼ê°€ ìˆì§€ë§Œ ì¼ë°˜ì  ì§ˆë¬¸ì´ë©´ â†’ ë‚´ë¶€ + ì™¸ë¶€ ê²€ìƒ‰
            try:
                external_result = search_naver_web(query, max_results=2)
                return f"ğŸ¢ **íšŒì‚¬ ë‚´ë¶€ ì •ë³´:**\n{internal_result}\n\n" + \
                       f"ğŸŒ **ì¼ë°˜ ì •ë³´ (ë„¤ì´ë²„ ê²€ìƒ‰):**\n{external_result}"
            except:
                return f"ğŸ¢ **íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:**\n\n{internal_result}"
                
        else:
            # ë‚´ë¶€ ë¬¸ì„œì— ê²°ê³¼ê°€ ì—†ìœ¼ë©´ â†’ ì™¸ë¶€ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
            try:
                external_result = search_naver_web(query, max_results=3)
                return f"ğŸ¢ **íšŒì‚¬ ë‚´ë¶€ ê²€ìƒ‰:** ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n" + \
                       f"ğŸŒ **ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰ ê²°ê³¼:**\n{external_result}"
            except:
                return f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {query}"
                
    except Exception as e:
        return f"í†µí•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

@tool  
def search_company_info(query: str) -> str:
    """íšŒì‚¬ ë‚´ë¶€ ì •ë³´ë§Œì„ ì „ìš©ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì¸ì‚¬ê·œì •, ì¡°ì§ì •ë³´, íšŒì‚¬ ì •ì±… ë“±ì„ ì°¾ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”."""
    try:
        result = rag_tool_instance.query_rag(query)
        if "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in result:
            return f"ğŸ¢ **íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:**\n\n'{query}'ì— ëŒ€í•œ íšŒì‚¬ ë‚´ë¶€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n" + \
                   "ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n" + \
                   " - ì¸ì‚¬ ê·œì • ë° ì •ì±…\n - ì¡°ì§ êµ¬ì¡° ë° ì—…ë¬´ ë¶„ì¥\n - ë³µë¦¬í›„ìƒ ì œë„\n - ê·¼ë¬´ ê·œì¹™"
        else:
            return f"ğŸ¢ **íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:**\n\n{result}"
    except Exception as e:
        return f"íšŒì‚¬ ë‚´ë¶€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}" 